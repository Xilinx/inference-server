
.. _program_listing_file__workspace_amdinfer_include_amdinfer_core_inference_tensor.hpp:

Program Listing for File inference_tensor.hpp
=============================================

|exhale_lsh| :ref:`Return to documentation for file <file__workspace_amdinfer_include_amdinfer_core_inference_tensor.hpp>` (``/workspace/amdinfer/include/amdinfer/core/inference_tensor.hpp``)

.. |exhale_lsh| unicode:: U+021B0 .. UPWARDS ARROW WITH TIP LEFTWARDS

.. code-block:: cpp

   // Copyright 2023 Advanced Micro Devices, Inc.
   //
   // Licensed under the Apache License, Version 2.0 (the "License");
   // you may not use this file except in compliance with the License.
   // You may obtain a copy of the License at
   //
   //      http://www.apache.org/licenses/LICENSE-2.0
   //
   // Unless required by applicable law or agreed to in writing, software
   // distributed under the License is distributed on an "AS IS" BASIS,
   // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   // See the License for the specific language governing permissions and
   // limitations under the License.
   
   #ifndef GUARD_AMDINFER_CORE_INFERENCE_TENSOR
   #define GUARD_AMDINFER_CORE_INFERENCE_TENSOR
   
   #include "amdinfer/core/parameters.hpp"  // for ParameterMap
   #include "amdinfer/core/tensor.hpp"      // IWYU pragma: export
   
   namespace amdinfer {
   
   class InferenceTensor : public Tensor {
    public:
     InferenceTensor(std::string name, std::vector<uint64_t> shape,
                     DataType data_type);
     explicit InferenceTensor(const Tensor &tensor);
   
     [[nodiscard]] const ParameterMap &getParameters() const &;
     [[nodiscard]] ParameterMap getParameters() &&;
   
     void setParameters(ParameterMap parameters);
   
     [[nodiscard]] size_t serializeSize() const override;
     std::byte *serialize(std::byte *data_out) const override;
     const std::byte *deserialize(const std::byte *data_in) override;
   
    private:
     ParameterMap parameters_;
   };
   
   }  // namespace amdinfer
   
   #endif  // GUARD_AMDINFER_CORE_INFERENCE_TENSOR
