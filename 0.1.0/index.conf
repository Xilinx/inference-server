<p>Xilinx Inference Server is an easy-to-use inferencing solution designed for Xilinx FPGAs and <a href="https://github.com/Xilinx/Vitis-AI">Vitis AI</a>. It can be deployed as a server or through custom applications using its C++ API. Xilinx Inference Server can also be extended to support other hardware accelerators and machine learning frameworks.</p>
<h2>Features</h2>
<ul>
<li>
<p>Inference Server: Xilinx Inference Server supports client requests using HTTP REST / websockets protocols using an API based on <a href="https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md">KServe’s v2 specification</a></p>
</li>
<li>
<p>C++ API: custom applications to directly call the backend to bypass the REST interface</p>
</li>
<li>
<p>Python REST library: clients can submit requests to the inference server from Python through this simplified Python API</p>
</li>
<li>
<p>Efficient hardware usage: Xilinx Inference Server will automatically make use of all available FPGAs on a machine as needed with <a href="https://github.com/Xilinx/XRM">XRM</a></p>
</li>
<li>
<p>User-defined model parallelism: users can define how many models, and how many instances of each, to run simultaneously</p>
</li>
<li>
<p>Batching: incoming requests are batched based on the model’s specifications transparently</p>
</li>
<li>
<p>Integrated with Vitis AI: Xilinx Inference Server can serve most xmodels generated from Vitis AI</p>
</li>
<li>
<p>End-to-end inference: A graph of computation such as pre- and post-processing can be written and deployed with Xilinx Inference Server using <a href="https://github.com/Xilinx/Vitis-AI/tree/master/tools/AKS">AKS</a></p>
</li>
</ul>
<h2>Learn more</h2>
<p>The documentation for Xilinx Inference Server is available <a href="https://xilinx.github.io/inference-server/">online</a>.</p>
<p>Check out the <a href="https://xilinx.github.io/inference-server/quickstart.html">Quickstart</a> on how to get started.</p>
<h2>Support</h2>
<p>Raise issues if you find a bug or need help. Refer to <a href="https://xilinx.github.io/inference-server/contributing.html">Contributing</a> for more information.</p>
<h2>License</h2>
<p>Xilinx Inference Server is licensed under the terms of Apache 2.0 (see <a href="https://github.com/Xilinx/inference-server/blob/main/LICENSE">LICENSE</a>). The LICENSE file contains additional license information for third-party files distributed with this work. More license information can be seen in the <a href="https://xilinx.github.io/inference-server/dependencies.html">dependencies</a>.</p>
