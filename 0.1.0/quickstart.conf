<p>The easiest way to use Xilinx Inference Server is to run it inside a Docker container. For these instructions, youâ€™ll need Git, Python3, and <a href="https://docs.docker.com/get-docker/">Docker</a>.</p>
<p>Note: the sequence of instructions in the code blocks below is valid only as written. If you add or change flags to commands, you may need to change the subsequent commands. The helper script used for most of the commands here is <code>proteus</code>. The most up-to-date documentation for this script can be seen with <ac:link ac:anchor="Command-LineInterface">
<ri:page ri:content-title="Command-Line Interface" />
<ac:link-body>online</ac:link-body>
</ac:link> or on the terminal with <code>--help</code>.</p>
<p>Some options also require <a href="https://docs.docker.com/compose/install/">Docker-Compose</a>. Running tests requires using Git LFS when cloning this repository to get testing assets.</p>
<h2>Build or Get the Docker Image</h2>
<p>We can build several types of containers:</p>
<div style="margin-left: 30px;padding-top: 10px;">
<ul>
<li>
<p>stable vs. nightly: stable containers use the latest released versions of the Vitis-AI libraries that are publicly available. Nightly containers use internal versions of these libraries and thus cannot be built by the general public.</p>
</li>
<li>
<p>development vs. production: dev containers are intended for working on Xilinx Inference Server or applications that link to Xilinx Inference Server. They include all the build dependencies and mount the working directory into the container and drop the user into a terminal when they start. Production containers only contain the runtime dependencies of the <strong>proteus-server</strong> executable and automatically run the executable when they start.</p>
</li>
</ul>
</div>
<p>Currently, these images are not pre-built anywhere and so must be built by the user. You must enable <a href="https://docs.docker.com/develop/develop-images/build_enhancements/">BuildKit</a> by setting DOCKER_BUILDKIT in the environment, configuring the Docker daemon or using <code>docker buildx install</code> before attempting to build the image. Refer to the BuildKit documentation for more information.</p>
<ac:structured-macro ac:name="code">
<ac:parameter ac:name="language">none</ac:parameter>
<ac:parameter ac:name="linenumbers">false</ac:parameter>
<ac:plain-text-body><![CDATA[$ ./proteus dockerize]]></ac:plain-text-body>
</ac:structured-macro>
<p>By default, this builds the stable dev image as <code>{username}/proteus-dev</code>. After the image is built, run the container:</p>
<ac:structured-macro ac:name="code">
<ac:parameter ac:name="language">none</ac:parameter>
<ac:parameter ac:name="linenumbers">false</ac:parameter>
<ac:plain-text-body><![CDATA[$ ./proteus run --dev]]></ac:plain-text-body>
</ac:structured-macro>
<p>This command runs the <code>{username}/proteus-dev:latest</code> image, which corresponds to the latest local dev image. The dev container will mount the working directory into <code>/workspace/proteus/</code>, mount some additional directories into the container, expose some ports to the host and pass in any available hardware like FPGAs. These details are the <code>--dev</code> profile. Some options may be overridden on the command-line (use <code>--help</code> to see the options).</p>
<h2>Building Xilinx Inference Server</h2>
<p>These commands are all run inside the dev container. Here, <code>./proteus</code> is aliased to <strong>proteus</strong>.</p>
<ac:structured-macro ac:name="code">
<ac:parameter ac:name="language">none</ac:parameter>
<ac:parameter ac:name="linenumbers">false</ac:parameter>
<ac:plain-text-body><![CDATA[$ proteus build --all]]></ac:plain-text-body>
</ac:structured-macro>
<p>The build command builds <strong>proteus-server</strong> as well as the AKS kernels and documentation. By default, this will be the debug version.</p>
<p>AKS is the <ac:link ac:anchor="AKS">
<ri:page ri:content-title="AKS" />
<ac:link-body>AI Kernel Scheduler</ac:link-body>
</ac:link> that may be used in Xilinx Inference Server. The AKS kernels need to be built prior to starting the server and requesting inferences from a worker that uses AKS. Subsequent builds can omit <code>--all</code> to skip rebuilding the AKS kernels.</p>
<h2>Getting Artifacts</h2>
<p>For running tests and certain examples, you may need to get artifacts such as test images and XModels.</p>
<ac:structured-macro ac:name="code">
<ac:parameter ac:name="language">none</ac:parameter>
<ac:parameter ac:name="linenumbers">false</ac:parameter>
<ac:plain-text-body><![CDATA[$ proteus get]]></ac:plain-text-body>
</ac:structured-macro>
<p>You must abide by the license agreements of these files, if you choose to download them.</p>
<h2>Running Xilinx Inference Server</h2>
<p>Once the server is built, start the server to begin serving requests.</p>
<ac:structured-macro ac:name="code">
<ac:parameter ac:name="language">bash</ac:parameter>
<ac:parameter ac:name="linenumbers">false</ac:parameter>
<ac:plain-text-body><![CDATA[# start proteus-server in the background
$ proteus start &

# test that the server is ready. The server returns status 200 OK on success
$ curl -I http://localhost:8998/v2/health/ready

# the server can now accept requests over REST

# shutdown the server
$ kill -2 $(pidof proteus-server)]]></ac:plain-text-body>
</ac:structured-macro>
<p>You can also try running the test suite. You may need to get testing artifacts (see above) and have cloned the repository with Git LFS enabled.</p>
<ac:structured-macro ac:name="code">
<ac:parameter ac:name="language">bash</ac:parameter>
<ac:parameter ac:name="linenumbers">false</ac:parameter>
<ac:plain-text-body><![CDATA[# this will start the server and test the REST API from Python.
$ proteus test]]></ac:plain-text-body>
</ac:structured-macro>
<p>Note: the facedetect test may fail due to a known issue with parsing the XModel. Ignore any failures with that model at this time.</p>
<p>Now that we can build and run the server, we will take a look at how to send requests to it using the Python API and link custom applications to Xilinx Inference Server using the C++ API.</p>
