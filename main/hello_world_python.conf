<p>Xilinx Inference Server’s Python API allows you to start the server and send requests to it from Python. This example walks you through how to start the server and use Python to send requests to a simple model. The complete script used here is available: <code>examples/python/hello_world_rest.py</code>.</p>
<h2>Import the library</h2>
<p>We need to bring in the Xilinx Inference Server Python library. The library’s source code is in <code>src/python</code> and it gets installed in the dev container on startup.</p>
<ac:structured-macro ac:name="code">
<ac:parameter ac:name="language">python</ac:parameter>
<ac:parameter ac:name="linenumbers">false</ac:parameter>
<ac:plain-text-body><![CDATA[import proteus]]></ac:plain-text-body>
</ac:structured-macro>
<h2>Create our client and server objects</h2>
<p>We assume that the server will be running locally with the default HTTP port and pass that to the client. In this example, we’ll be using REST to communicate to the server so we create a <code>RestClient</code> object.</p>
<ac:structured-macro ac:name="code">
<ac:parameter ac:name="language">python</ac:parameter>
<ac:parameter ac:name="linenumbers">false</ac:parameter>
<ac:plain-text-body><![CDATA[server = proteus.Server()
client = proteus.RestClient("127.0.0.1:8998")]]></ac:plain-text-body>
</ac:structured-macro>
<h2>Is Xilinx Inference Server already running?</h2>
<p>If the server is already started externally, we don’t want to start it again. So, we attempt to check if the server is live. If this fails, we start the server ourselves. Either way, the client will attempt to communicate to the server at <code>http://localhost:8998</code>.</p>
<ac:structured-macro ac:name="code">
<ac:parameter ac:name="language">python</ac:parameter>
<ac:parameter ac:name="linenumbers">false</ac:parameter>
<ac:plain-text-body><![CDATA[try:
    start_server = not client.server_live()
except proteus.ConnectionError:
    start_server = True
if start_server:
    server.start(quiet=True)
    client.wait_until_live()]]></ac:plain-text-body>
</ac:structured-macro>
<h2>Load a worker</h2>
<p>Inference requests in Xilinx Inference Server are made to workers. Workers are started as threads in Xilinx Inference Server and have a defined lifecycle. Before making an inference request to a worker, it must first be loaded. Loading a worker returns an identifier that the client should use for future operations.</p>
<p>This worker, Echo, is a simple example worker that accepts integers as input, adds one to the inputs and returns the sum. We’ll use it to demonstrate the Python flow.</p>
<ac:structured-macro ac:name="code">
<ac:parameter ac:name="language">python</ac:parameter>
<ac:parameter ac:name="linenumbers">false</ac:parameter>
<ac:plain-text-body><![CDATA[response = client.load("Echo")
assert not response.error, response.error_msg
worker_name = response.html

while not client.model_ready(worker_name):
    pass]]></ac:plain-text-body>
</ac:structured-macro>
<h2>Inference</h2>
<p>Once the worker is ready, we can make an inference request to it. We construct a request that contains five integers and send it to Xilinx Inference Server. The <code>NumericalInferenceRequest</code> class is a helper class that simplifies creating a request in the right format.</p>
<ac:structured-macro ac:name="code">
<ac:parameter ac:name="language">python</ac:parameter>
<ac:parameter ac:name="linenumbers">false</ac:parameter>
<ac:plain-text-body><![CDATA[data = [3, 1, 4, 1, 5]
request = proteus.NumericalInferenceRequest(data)
response = client.infer(worker_name, request)]]></ac:plain-text-body>
</ac:structured-macro>
<h2>Validate the response</h2>
<p>Now, we want to check what our response is. Here, we can simplify our checks because we already know what we expect to receive. So we check is that the number of outputs match the number of inputs we used. We also check that each output only has one index and is one more than the corresponding input since we know that’s what the Echo worker does.</p>
<ac:structured-macro ac:name="code">
<ac:parameter ac:name="language">python</ac:parameter>
<ac:parameter ac:name="linenumbers">false</ac:parameter>
<ac:plain-text-body><![CDATA[assert not response.error, response.error_msg
assert len(response.outputs) == len(data)
for index, output in enumerate(response.outputs):
    assert len(output.data) == 1
    assert output.data[0] == data[index] + 1]]></ac:plain-text-body>
</ac:structured-macro>
<h2>Clean up</h2>
<p>Workers that are loaded in Xilinx Inference Server will persist until the server shuts down or they’re explicitly unloaded. While it’s not shown here, the Python API provides an <code>unload()</code> method for this purpose. Finally, if we started the server from Python, we shut it down before finishing. If there are any loaded workers at this time, they will be cleaned up before shutdown.</p>
<ac:structured-macro ac:name="code">
<ac:parameter ac:name="language">python</ac:parameter>
<ac:parameter ac:name="linenumbers">false</ac:parameter>
<ac:plain-text-body><![CDATA[if start_server:
    server.stop()
    client.wait_until_stop()]]></ac:plain-text-body>
</ac:structured-macro>
