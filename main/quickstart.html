<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
<!-- OneTrust Cookies Consent Notice start for xilinx.github.io -->

<script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-document-language="true" type="text/javascript" charset="UTF-8" data-domain-script="03af8d57-0a04-47a6-8f10-322fa00d8fc7" ></script>
<script type="text/javascript">
function OptanonWrapper() { }
</script>
<!-- OneTrust Cookies Consent Notice end for xilinx.github.io -->
<!-- Google Tag Manager -->
<script type="text/plain" class="optanon-category-C0002">(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-5RHQV7');</script>
<!-- End Google Tag Manager -->
  <title>Quickstart &mdash; AMD Inference Server main documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/twemoji.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
      <link rel="stylesheet" href="_static/tabs.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="https://twemoji.maxcdn.com/v/latest/twemoji.min.js"></script>
        <script src="_static/twemoji.js"></script>
        <script src="_static/tabs.js"></script>
        <script defer="defer" src="https://unpkg.com/@popperjs/core@2"></script>
        <script defer="defer" src="https://unpkg.com/tippy.js@6"></script>
        <script defer="defer" src="_static/tippy/quickstart.25fa6d2d-1c1b-41b2-b260-376192ef678a.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Backends" href="backends.html" />
    <link rel="prev" title="Terminology" href="terminology.html" /> 
</head>

<body class="wy-body-for-nav">

<!-- Google Tag Manager -->
<noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-5RHQV7" height="0" width="0" style="display:none;visibility:hidden" class="optanon-category-C0002"></iframe></noscript>
<!-- End Google Tag Manager --> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
            <a href="index.html" class="icon icon-home"> AMD Inference Server
          </a>
              <div class="version">
                main
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting started</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="introduction.html#features">Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="introduction.html#documentation-overview">Documentation overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="introduction.html#support">Support</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="terminology.html">Terminology</a><ul>
<li class="toctree-l2"><a class="reference internal" href="terminology.html#amdinfer">amdinfer</a></li>
<li class="toctree-l2"><a class="reference internal" href="terminology.html#types-of-docker-images">Types of Docker images</a><ul>
<li class="toctree-l3"><a class="reference internal" href="terminology.html#development">Development</a></li>
<li class="toctree-l3"><a class="reference internal" href="terminology.html#deployment">Deployment</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="terminology.html#types-of-users">Types of users</a><ul>
<li class="toctree-l3"><a class="reference internal" href="terminology.html#clients">Clients</a></li>
<li class="toctree-l3"><a class="reference internal" href="terminology.html#administrators">Administrators</a></li>
<li class="toctree-l3"><a class="reference internal" href="terminology.html#developers">Developers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Quickstart</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#prerequisites">Prerequisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="#prepare-the-model-repository">Prepare the model repository</a></li>
<li class="toctree-l2"><a class="reference internal" href="#get-the-deployment-image">Get the deployment image</a></li>
<li class="toctree-l2"><a class="reference internal" href="#start-the-image">Start the image</a></li>
<li class="toctree-l2"><a class="reference internal" href="#server-deployment-summary">Server deployment summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="#get-the-python-library">Get the Python library</a></li>
<li class="toctree-l2"><a class="reference internal" href="#running-an-example">Running an example</a></li>
<li class="toctree-l2"><a class="reference internal" href="#inference-summary">Inference summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="#next-steps">Next steps</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="backends.html">Backends</a><ul>
<li class="toctree-l2"><a class="reference internal" href="backends/cplusplus.html">CPlusPlus</a><ul>
<li class="toctree-l3"><a class="reference internal" href="backends/cplusplus.html#model-support">Model support</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/cplusplus.html#hardware-support">Hardware support</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/cplusplus.html#host-setup">Host setup</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/cplusplus.html#build-an-image">Build an image</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/cplusplus.html#start-a-container">Start a container</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/cplusplus.html#get-test-assets">Get test assets</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/cplusplus.html#loading-the-backend">Loading the backend</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/cplusplus.html#troubleshooting">Troubleshooting</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="backends/migraphx.html">MIGraphX</a><ul>
<li class="toctree-l3"><a class="reference internal" href="backends/migraphx.html#model-support">Model support</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/migraphx.html#hardware-support">Hardware support</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/migraphx.html#host-setup">Host setup</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/migraphx.html#build-an-image">Build an image</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/migraphx.html#start-a-container">Start a container</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/migraphx.html#get-test-assets">Get test assets</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/migraphx.html#loading-the-backend">Loading the backend</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/migraphx.html#troubleshooting">Troubleshooting</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="backends/ptzendnn.html">PtZenDNN</a><ul>
<li class="toctree-l3"><a class="reference internal" href="backends/ptzendnn.html#model-support">Model support</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/ptzendnn.html#hardware-support">Hardware support</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/ptzendnn.html#host-setup">Host setup</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/ptzendnn.html#build-an-image">Build an image</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/ptzendnn.html#start-a-container">Start a container</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/ptzendnn.html#get-test-assets">Get test assets</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/ptzendnn.html#loading-the-backend">Loading the backend</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/ptzendnn.html#troubleshooting">Troubleshooting</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="backends/tfzendnn.html">TfZenDNN</a><ul>
<li class="toctree-l3"><a class="reference internal" href="backends/tfzendnn.html#model-support">Model support</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/tfzendnn.html#hardware-support">Hardware support</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/tfzendnn.html#host-setup">Host setup</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/tfzendnn.html#build-an-image">Build an image</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/tfzendnn.html#start-a-container">Start a container</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/tfzendnn.html#get-test-assets">Get test assets</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/tfzendnn.html#loading-the-backend">Loading the backend</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/tfzendnn.html#troubleshooting">Troubleshooting</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="backends/vitis_ai.html">Vitis AI</a><ul>
<li class="toctree-l3"><a class="reference internal" href="backends/vitis_ai.html#model-support">Model support</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/vitis_ai.html#hardware-support">Hardware support</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/vitis_ai.html#host-setup">Host setup</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/vitis_ai.html#build-an-image">Build an image</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/vitis_ai.html#start-a-container">Start a container</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/vitis_ai.html#get-test-assets">Get test assets</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/vitis_ai.html#loading-the-backend">Loading the backend</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/vitis_ai.html#troubleshooting">Troubleshooting</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="model_repository.html">Model Repository</a><ul>
<li class="toctree-l2"><a class="reference internal" href="model_repository.html#single-models">Single models</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_repository.html#ensembles">Ensembles</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ensembles.html">Ensembles</a><ul>
<li class="toctree-l2"><a class="reference internal" href="ensembles.html#defining-ensembles">Defining ensembles</a><ul>
<li class="toctree-l3"><a class="reference internal" href="ensembles.html#model-repository">Model repository</a></li>
<li class="toctree-l3"><a class="reference internal" href="ensembles.html#api">API</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="deployment.html">Deployment</a><ul>
<li class="toctree-l2"><a class="reference internal" href="deployment.html#deployment-image">Deployment image</a><ul>
<li class="toctree-l3"><a class="reference internal" href="deployment.html#get-the-image">Get the image</a></li>
<li class="toctree-l3"><a class="reference internal" href="deployment.html#build-the-image">Build the image</a></li>
<li class="toctree-l3"><a class="reference internal" href="deployment.html#push-to-a-registry">Push to a registry</a></li>
<li class="toctree-l3"><a class="reference internal" href="deployment.html#prepare-the-image">Prepare the image</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="deployment.html#start-a-container">Start a container</a></li>
<li class="toctree-l2"><a class="reference internal" href="deployment.html#kserve">KServe</a></li>
<li class="toctree-l2"><a class="reference internal" href="deployment.html#development-image">Development image</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="kserve.html">KServe</a><ul>
<li class="toctree-l2"><a class="reference internal" href="kserve.html#set-up-kubernetes-and-kserve">Set up Kubernetes and KServe</a></li>
<li class="toctree-l2"><a class="reference internal" href="kserve.html#get-or-build-the-amd-inference-server-image">Get or build the AMD Inference Server Image</a></li>
<li class="toctree-l2"><a class="reference internal" href="kserve.html#start-an-inference-service">Start an inference service</a><ul>
<li class="toctree-l3"><a class="reference internal" href="kserve.html#serving-runtime">Serving Runtime</a></li>
<li class="toctree-l3"><a class="reference internal" href="kserve.html#custom-container">Custom container</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="kserve.html#making-requests">Making Requests</a></li>
<li class="toctree-l2"><a class="reference internal" href="kserve.html#debugging">Debugging</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="performance_factors.html">Performance Factors</a><ul>
<li class="toctree-l2"><a class="reference internal" href="performance_factors.html#hardware">Hardware</a></li>
<li class="toctree-l2"><a class="reference internal" href="performance_factors.html#compile-the-right-version">Compile the right version</a></li>
<li class="toctree-l2"><a class="reference internal" href="performance_factors.html#parallelism">Parallelism</a><ul>
<li class="toctree-l3"><a class="reference internal" href="performance_factors.html#rest-threads">REST threads</a></li>
<li class="toctree-l3"><a class="reference internal" href="performance_factors.html#sending-requests">Sending requests</a></li>
<li class="toctree-l3"><a class="reference internal" href="performance_factors.html#duplicating-workers">Duplicating workers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="troubleshooting.html">Troubleshooting</a><ul>
<li class="toctree-l2"><a class="reference internal" href="troubleshooting.html#use-the-latest-version">Use the latest version</a></li>
<li class="toctree-l2"><a class="reference internal" href="troubleshooting.html#use-server-logs">Use server logs</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="hello_world_echo.html">Hello World - Python</a><ul>
<li class="toctree-l2"><a class="reference internal" href="hello_world_echo.html#import-the-library">Import the library</a></li>
<li class="toctree-l2"><a class="reference internal" href="hello_world_echo.html#create-our-client-and-server-objects">Create our client and server objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="hello_world_echo.html#is-amd-inference-server-already-running">Is AMD Inference Server already running?</a></li>
<li class="toctree-l2"><a class="reference internal" href="hello_world_echo.html#load-a-worker">Load a worker</a></li>
<li class="toctree-l2"><a class="reference internal" href="hello_world_echo.html#inference">Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="hello_world_echo.html#validate-the-response">Validate the response</a></li>
<li class="toctree-l2"><a class="reference internal" href="hello_world_echo.html#clean-up">Clean up</a></li>
<li class="toctree-l2"><a class="reference internal" href="hello_world_echo.html#next-steps">Next steps</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="example_resnet50_cpp.html">Running ResNet50 - C++</a><ul>
<li class="toctree-l2"><a class="reference internal" href="example_resnet50_cpp.html#include-the-header">Include the header</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_resnet50_cpp.html#start-the-server">Start the server</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_resnet50_cpp.html#create-the-client-object">Create the client object</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_resnet50_cpp.html#load-a-worker">Load a worker</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_resnet50_cpp.html#prepare-images">Prepare images</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_resnet50_cpp.html#construct-requests">Construct requests</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_resnet50_cpp.html#make-an-inference">Make an inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="example_resnet50_python.html">Running ResNet50 - Python</a><ul>
<li class="toctree-l2"><a class="reference internal" href="example_resnet50_python.html#include-the-module">Include the module</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_resnet50_python.html#start-the-server">Start the server</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_resnet50_python.html#create-the-client-object">Create the client object</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_resnet50_python.html#load-a-worker">Load a worker</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_resnet50_python.html#prepare-images">Prepare images</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_resnet50_python.html#construct-requests">Construct requests</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_resnet50_python.html#make-an-inference">Make an inference</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quickstart_development.html">Developer Quickstart</a><ul>
<li class="toctree-l2"><a class="reference internal" href="quickstart_development.html#prerequisites">Prerequisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart_development.html#set-up-the-host">Set up the host</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart_development.html#get-the-code">Get the code</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart_development.html#amdinfer-script">amdinfer script</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart_development.html#build-or-get-the-docker-image">Build or get the Docker image</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart_development.html#compiling-the-amd-inference-server">Compiling the AMD Inference Server</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart_development.html#get-test-artifacts">Get test artifacts</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart_development.html#run-the-amd-inference-server">Run the AMD Inference Server</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart_development.html#next-steps">Next steps</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">Testing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="testing.html#add-a-new-test">Add a new test</a><ul>
<li class="toctree-l3"><a class="reference internal" href="testing.html#add-assets">Add assets</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Architecture</a><ul>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#ingestion">Ingestion</a><ul>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#api">API</a></li>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#http-rest-and-websocket">HTTP/REST and WebSocket</a></li>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#grpc">gRPC</a></li>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#c-api">C++ API</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#batching">Batching</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#workers">Workers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#organization-and-lifecycle">Organization and Lifecycle</a></li>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#improving-performance">Improving Performance</a></li>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#external-processing">External Processing</a></li>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#xmodel">XModel</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#shared-state">Shared State</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#observation">Observation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#logging">Logging</a></li>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#metrics">Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#tracing">Tracing</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="aks.html">AKS</a><ul>
<li class="toctree-l2"><a class="reference internal" href="aks.html#introduction-to-aks">Introduction to AKS</a></li>
<li class="toctree-l2"><a class="reference internal" href="aks.html#using-aks-in-amd-inference-server">Using AKS in AMD Inference Server</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">Logs</a><ul>
<li class="toctree-l2"><a class="reference internal" href="logging.html#amd-inference-server-logs">AMD Inference Server Logs</a></li>
<li class="toctree-l2"><a class="reference internal" href="logging.html#drogon-logs">Drogon Logs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="benchmarking.html">Benchmarking</a><ul>
<li class="toctree-l2"><a class="reference internal" href="benchmarking.html#xmodel-benchmarking">XModel Benchmarking</a></li>
<li class="toctree-l2"><a class="reference internal" href="benchmarking.html#kernel-simulation">Kernel Simulation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="metrics.html">Metrics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="metrics.html#quickstart">Quickstart</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tracing.html">Tracing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="tracing.html#quickstart">Quickstart</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">About</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#ways-to-contribute">Ways to contribute</a><ul>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#idea-generation">Idea generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#raise-issues">Raise issues</a></li>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#triage">Triage</a></li>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#raise-pull-requests">Raise pull requests</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#style-guide">Style guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#documentation">Documentation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="dependencies.html">Dependencies</a><ul>
<li class="toctree-l2"><a class="reference internal" href="dependencies.html#docker-image">Docker Image</a><ul>
<li class="toctree-l3"><a class="reference internal" href="dependencies.html#base-image">Base Image</a></li>
<li class="toctree-l3"><a class="reference internal" href="dependencies.html#ubuntu-focal-repositories">Ubuntu Focal Repositories</a></li>
<li class="toctree-l3"><a class="reference internal" href="dependencies.html#ubuntu-ppas">Ubuntu PPAs</a></li>
<li class="toctree-l3"><a class="reference internal" href="dependencies.html#pypi">PyPI</a></li>
<li class="toctree-l3"><a class="reference internal" href="dependencies.html#github">Github</a></li>
<li class="toctree-l3"><a class="reference internal" href="dependencies.html#others">Others</a></li>
<li class="toctree-l3"><a class="reference internal" href="dependencies.html#xilinx">Xilinx</a></li>
<li class="toctree-l3"><a class="reference internal" href="dependencies.html#amd">AMD</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="dependencies.html#included">Included</a></li>
<li class="toctree-l2"><a class="reference internal" href="dependencies.html#downloaded-files">Downloaded Files</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="changelog.html">Changelog</a><ul>
<li class="toctree-l2"><a class="reference internal" href="changelog.html#unreleased">Unreleased</a><ul>
<li class="toctree-l3"><a class="reference internal" href="changelog.html#added">Added</a></li>
<li class="toctree-l3"><a class="reference internal" href="changelog.html#changed">Changed</a></li>
<li class="toctree-l3"><a class="reference internal" href="changelog.html#deprecated">Deprecated</a></li>
<li class="toctree-l3"><a class="reference internal" href="changelog.html#removed">Removed</a></li>
<li class="toctree-l3"><a class="reference internal" href="changelog.html#fixed">Fixed</a></li>
<li class="toctree-l3"><a class="reference internal" href="changelog.html#security">Security</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="changelog.html#id2">0.3.0 - 2023-02-01</a><ul>
<li class="toctree-l3"><a class="reference internal" href="changelog.html#id3">Added</a></li>
<li class="toctree-l3"><a class="reference internal" href="changelog.html#id4">Changed</a></li>
<li class="toctree-l3"><a class="reference internal" href="changelog.html#id5">Deprecated</a></li>
<li class="toctree-l3"><a class="reference internal" href="changelog.html#id6">Removed</a></li>
<li class="toctree-l3"><a class="reference internal" href="changelog.html#id7">Fixed</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="changelog.html#id8">0.2.0 - 2022-08-05</a><ul>
<li class="toctree-l3"><a class="reference internal" href="changelog.html#id9">Added</a></li>
<li class="toctree-l3"><a class="reference internal" href="changelog.html#id10">Changed</a></li>
<li class="toctree-l3"><a class="reference internal" href="changelog.html#id11">Fixed</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="changelog.html#id12">0.1.0 - 2022-02-08</a><ul>
<li class="toctree-l3"><a class="reference internal" href="changelog.html#id13">Added</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="roadmap.html">Roadmap</a><ul>
<li class="toctree-l2"><a class="reference internal" href="roadmap.html#id1">2022</a><ul>
<li class="toctree-l3"><a class="reference internal" href="roadmap.html#q1">2022 Q1</a></li>
<li class="toctree-l3"><a class="reference internal" href="roadmap.html#q2">2022 Q2</a></li>
<li class="toctree-l3"><a class="reference internal" href="roadmap.html#q3">2022 Q3</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="roadmap.html#id2">2023</a><ul>
<li class="toctree-l3"><a class="reference internal" href="roadmap.html#id3">2023 Q1</a></li>
<li class="toctree-l3"><a class="reference internal" href="roadmap.html#id4">2023 Q2</a></li>
<li class="toctree-l3"><a class="reference internal" href="roadmap.html#id5">2023 Q3</a></li>
<li class="toctree-l3"><a class="reference internal" href="roadmap.html#q4">2023 Q4</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="roadmap.html#future">Future</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries and API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="amdinfer_script.html">amdinfer Script</a><ul>
<li class="toctree-l2"><a class="reference internal" href="amdinfer_script.html#commands">Commands</a></li>
<li class="toctree-l2"><a class="reference internal" href="amdinfer_script.html#options">Options</a></li>
<li class="toctree-l2"><a class="reference internal" href="amdinfer_script.html#Sub-commands">Sub-commands</a><ul>
<li class="toctree-l3"><a class="reference internal" href="amdinfer_script.html#attach">attach</a></li>
<li class="toctree-l3"><a class="reference internal" href="amdinfer_script.html#benchmark">benchmark</a></li>
<li class="toctree-l3"><a class="reference internal" href="amdinfer_script.html#build">build</a></li>
<li class="toctree-l3"><a class="reference internal" href="amdinfer_script.html#clean">clean</a></li>
<li class="toctree-l3"><a class="reference internal" href="amdinfer_script.html#dockerize">dockerize</a></li>
<li class="toctree-l3"><a class="reference internal" href="amdinfer_script.html#get">get</a></li>
<li class="toctree-l3"><a class="reference internal" href="amdinfer_script.html#install">install</a></li>
<li class="toctree-l3"><a class="reference internal" href="amdinfer_script.html#list">list</a></li>
<li class="toctree-l3"><a class="reference internal" href="amdinfer_script.html#make">make</a></li>
<li class="toctree-l3"><a class="reference internal" href="amdinfer_script.html#run">run</a></li>
<li class="toctree-l3"><a class="reference internal" href="amdinfer_script.html#start">start</a></li>
<li class="toctree-l3"><a class="reference internal" href="amdinfer_script.html#test">test</a></li>
<li class="toctree-l3"><a class="reference internal" href="amdinfer_script.html#up">up</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="cpp_user_api.html">C++</a><ul>
<li class="toctree-l2"><a class="reference internal" href="cpp_user_api.html#clients">Clients</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cpp_user_api.html#grpc">gRPC</a></li>
<li class="toctree-l3"><a class="reference internal" href="cpp_user_api.html#http">HTTP</a></li>
<li class="toctree-l3"><a class="reference internal" href="cpp_user_api.html#native">Native</a></li>
<li class="toctree-l3"><a class="reference internal" href="cpp_user_api.html#websocket">WebSocket</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="cpp_user_api.html#core">Core</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cpp_user_api.html#datatype">DataType</a></li>
<li class="toctree-l3"><a class="reference internal" href="cpp_user_api.html#exceptions">Exceptions</a></li>
<li class="toctree-l3"><a class="reference internal" href="cpp_user_api.html#prediction">Prediction</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="cpp_user_api.html#servers">Servers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="python.html">Python</a><ul>
<li class="toctree-l2"><a class="reference internal" href="python.html#install-the-python-library">Install the Python library</a><ul>
<li class="toctree-l3"><a class="reference internal" href="python.html#build-wheels">Build wheels</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="python.html#module-amdinfer">API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.BadStatus"><code class="docutils literal notranslate"><span class="pre">BadStatus</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.Client"><code class="docutils literal notranslate"><span class="pre">Client</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.ConnectionError"><code class="docutils literal notranslate"><span class="pre">ConnectionError</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.DataType"><code class="docutils literal notranslate"><span class="pre">DataType</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.EnvironmentNotSetError"><code class="docutils literal notranslate"><span class="pre">EnvironmentNotSetError</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.ExternalError"><code class="docutils literal notranslate"><span class="pre">ExternalError</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.FileNotFoundError"><code class="docutils literal notranslate"><span class="pre">FileNotFoundError</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.FileReadError"><code class="docutils literal notranslate"><span class="pre">FileReadError</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.GrpcClient"><code class="docutils literal notranslate"><span class="pre">GrpcClient</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.HttpClient"><code class="docutils literal notranslate"><span class="pre">HttpClient</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.ImageInferenceRequest"><code class="docutils literal notranslate"><span class="pre">ImageInferenceRequest()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.InferenceRequest"><code class="docutils literal notranslate"><span class="pre">InferenceRequest</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.InferenceRequestInput"><code class="docutils literal notranslate"><span class="pre">InferenceRequestInput</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.InferenceRequestOutput"><code class="docutils literal notranslate"><span class="pre">InferenceRequestOutput</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.InferenceResponse"><code class="docutils literal notranslate"><span class="pre">InferenceResponse</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.InferenceResponseOutput"><code class="docutils literal notranslate"><span class="pre">InferenceResponseOutput</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.InferenceTensor"><code class="docutils literal notranslate"><span class="pre">InferenceTensor</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.InvalidArgumentError"><code class="docutils literal notranslate"><span class="pre">InvalidArgumentError</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.ModelMetadata"><code class="docutils literal notranslate"><span class="pre">ModelMetadata</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.NativeClient"><code class="docutils literal notranslate"><span class="pre">NativeClient</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.ParameterMap"><code class="docutils literal notranslate"><span class="pre">ParameterMap</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.RuntimeError"><code class="docutils literal notranslate"><span class="pre">RuntimeError</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.Server"><code class="docutils literal notranslate"><span class="pre">Server</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.ServerMetadata"><code class="docutils literal notranslate"><span class="pre">ServerMetadata</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.Tensor"><code class="docutils literal notranslate"><span class="pre">Tensor</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.WebSocketClient"><code class="docutils literal notranslate"><span class="pre">WebSocketClient</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.inferAsyncOrdered"><code class="docutils literal notranslate"><span class="pre">inferAsyncOrdered()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.inferAsyncOrderedBatched"><code class="docutils literal notranslate"><span class="pre">inferAsyncOrderedBatched()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.inference_request_to_dict"><code class="docutils literal notranslate"><span class="pre">inference_request_to_dict()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.loadEnsemble"><code class="docutils literal notranslate"><span class="pre">loadEnsemble()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.parallel_infer"><code class="docutils literal notranslate"><span class="pre">parallel_infer()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.serverHasExtension"><code class="docutils literal notranslate"><span class="pre">serverHasExtension()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.start_http_client_server"><code class="docutils literal notranslate"><span class="pre">start_http_client_server()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.stringToArray"><code class="docutils literal notranslate"><span class="pre">stringToArray()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.unloadModels"><code class="docutils literal notranslate"><span class="pre">unloadModels()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.waitUntilModelNotReady"><code class="docutils literal notranslate"><span class="pre">waitUntilModelNotReady()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.waitUntilModelReady"><code class="docutils literal notranslate"><span class="pre">waitUntilModelReady()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="python.html#amdinfer.waitUntilServerReady"><code class="docutils literal notranslate"><span class="pre">waitUntilServerReady()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="rest.html">REST Endpoints</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">AMD Inference Server</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Quickstart</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/quickstart.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="quickstart">
<h1>Quickstart<a class="headerlink" href="#quickstart" title="Permalink to this heading">¶</a></h1>
<p>This quickstart shows you how to deploy the server locally and send inference requests to it.
These steps elaborate on the quick start example described in the project <a class="reference external" href="https://github.com/Xilinx/inference-server#quick-start-deployment-and-inference">README</a>.
The README example uses a script that takes care of the setup that is manually performed here but this guide should help you better understand what is needed to deploy the server and use it.
If you already have the server deployed somewhere, you can skip ahead to the <a class="reference internal" href="#deploymentsummary"><span class="std std-ref">inference section</span></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This guide shows you how to get started quickly.
It is not intended to demonstrate all the different options and settings that could be applied at different stages.</p>
</div>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this heading">¶</a></h2>
<p>For making an inference:</p>
<ul class="simple">
<li><p>A Linux host machine with Python 3.6+</p></li>
</ul>
<p>For deploying the server:</p>
<ul class="simple">
<li><p>A Linux host machine with <a class="reference external" href="https://docs.docker.com/engine/install/">Docker</a> installed</p></li>
<li><p>Sufficient disk space to host your models</p></li>
</ul>
<p>This guide assumes that inference will be made from the same machine where the server is deployed.</p>
</section>
<section id="prepare-the-model-repository">
<h2>Prepare the model repository<a class="headerlink" href="#prepare-the-model-repository" title="Permalink to this heading">¶</a></h2>
<p>A model repository is a directory that exists on the host machine where the server <a class="reference internal" href="glossary.html#term-Container-Docker"><span class="xref std std-term">container</span></a> is running and it holds the models you want to serve and their associated metadata in a standard structure.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>/
├─ resnet50/
│  ├─ 1/
│  │  ├─ &lt;model&gt;
│  ├─ config.toml
</pre></div>
</div>
<p>The model name, <code class="docutils literal notranslate"><span class="pre">resnet50</span></code> in this template, must be unique among the models loaded on a particular server and it must match the name used in the configuration TOML file.
This name is used to name the endpoint that clients use to make inference requests.
Under this directory, there must be a directory named <code class="docutils literal notranslate"><span class="pre">1/</span></code> containing the model file itself and a TOML file containing the metadata for the model.
This TOML file may be named anything but <code class="docutils literal notranslate"><span class="pre">config.toml</span></code> is suggested and used throughout this documentation.
The model file can also have an arbitrary name and the file extension depends on the type of the model.</p>
<p>In this guide, you will deploy a single ResNet50 model.
You can create a model repository in your current working directory using these steps.
Depending on what hardware you want to use and have on your host machine, you can use the appropriate model.
The CPU version has no special hardware requirements to run so you can always run that.</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-0-Q1BV" aria-selected="true" class="sphinx-tabs-tab code-tab group-tab" id="tab-0-Q1BV" name="Q1BV" role="tab" tabindex="0">CPU</button><button aria-controls="panel-0-R1BV" aria-selected="false" class="sphinx-tabs-tab code-tab group-tab" id="tab-0-R1BV" name="R1BV" role="tab" tabindex="-1">GPU</button><button aria-controls="panel-0-RlBHQQ==" aria-selected="false" class="sphinx-tabs-tab code-tab group-tab" id="tab-0-RlBHQQ==" name="RlBHQQ==" role="tab" tabindex="-1">FPGA</button></div><div aria-labelledby="tab-0-Q1BV" class="sphinx-tabs-panel code-tab group-tab" id="panel-0-Q1BV" name="Q1BV" role="tabpanel" tabindex="0"><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>wget<span class="w"> </span>-O<span class="w"> </span>tensorflow.zip<span class="w"> </span>https://www.xilinx.com/bin/public/openDownload?filename<span class="o">=</span>tf_resnetv1_50_imagenet_224_224_6.97G_2.5.zip
<span class="gp">$ </span>unzip<span class="w"> </span>-j<span class="w"> </span><span class="s2">&quot;tensorflow.zip&quot;</span><span class="w"> </span><span class="s2">&quot;tf_resnetv1_50_imagenet_224_224_6.97G_2.5/float/resnet_v1_50_baseline_6.96B_922.pb&quot;</span><span class="w"> </span>-d<span class="w"> </span>.
<span class="gp">$ </span>mkdir<span class="w"> </span>-p<span class="w"> </span>./model_repository/resnet50/1
<span class="gp">$ </span>mv<span class="w"> </span>./resnet_v1_50_baseline_6.96B_922.pb<span class="w"> </span>./model_repository/resnet50/1/
</pre></div>
</div>
</div><div aria-labelledby="tab-0-R1BV" class="sphinx-tabs-panel code-tab group-tab" hidden="true" id="panel-0-R1BV" name="R1BV" role="tabpanel" tabindex="0"><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>wget<span class="w"> </span>https://github.com/onnx/models/raw/main/vision/classification/resnet/model/resnet50-v2-7.onnx
<span class="gp">$ </span>mkdir<span class="w"> </span>-p<span class="w"> </span>./model_repository/resnet50/1
<span class="gp">$ </span>mv<span class="w"> </span>./resnet50-v2-7.onnx<span class="w"> </span>./model_repository/resnet50/1/
</pre></div>
</div>
</div><div aria-labelledby="tab-0-RlBHQQ==" class="sphinx-tabs-panel code-tab group-tab" hidden="true" id="panel-0-RlBHQQ==" name="RlBHQQ==" role="tabpanel" tabindex="0"><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>wget<span class="w"> </span>-O<span class="w"> </span>vitis.tar.gz<span class="w"> </span>https://www.xilinx.com/bin/public/openDownload?filename<span class="o">=</span>resnet_v1_50_tf-u200-u250-r2.5.0.tar.gz
<span class="gp">$ </span>tar<span class="w"> </span>-xzf<span class="w"> </span>vitis.tar.gz<span class="w"> </span><span class="s2">&quot;resnet_v1_50_tf/resnet_v1_50_tf.xmodel&quot;</span>
<span class="gp">$ </span>mkdir<span class="w"> </span>-p<span class="w"> </span>./model_repository/resnet50/1
<span class="gp">$ </span>mv<span class="w"> </span>./resnet_v1_50_tf/resnet_v1_50_tf.xmodel<span class="w"> </span>./model_repository/resnet50/1/
</pre></div>
</div>
</div></div>
<p>For the models used here, their corresponding <code class="docutils literal notranslate"><span class="pre">config.toml</span></code> should be placed in the chosen model repository (<code class="docutils literal notranslate"><span class="pre">./model_repository/resnet50/</span></code>):</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-1-Q1BV" aria-selected="true" class="sphinx-tabs-tab code-tab group-tab" id="tab-1-Q1BV" name="Q1BV" role="tab" tabindex="0">CPU</button><button aria-controls="panel-1-R1BV" aria-selected="false" class="sphinx-tabs-tab code-tab group-tab" id="tab-1-R1BV" name="R1BV" role="tab" tabindex="-1">GPU</button><button aria-controls="panel-1-RlBHQQ==" aria-selected="false" class="sphinx-tabs-tab code-tab group-tab" id="tab-1-RlBHQQ==" name="RlBHQQ==" role="tab" tabindex="-1">FPGA</button></div><div aria-labelledby="tab-1-Q1BV" class="sphinx-tabs-panel code-tab group-tab" id="panel-1-Q1BV" name="Q1BV" role="tabpanel" tabindex="0"><div class="highlight-toml notranslate"><div class="highlight"><pre><span></span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;resnet50&quot;</span>
<span class="n">platform</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;tensorflow_graphdef&quot;</span>

<span class="k">[[inputs]]</span>
<span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;input&quot;</span>
<span class="n">datatype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;FP32&quot;</span>
<span class="n">shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="mi">224</span><span class="p">,</span><span class="w"> </span><span class="mi">224</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">]</span>

<span class="k">[[outputs]]</span>
<span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;resnet_v1_50/predictions/Reshape_1&quot;</span>
<span class="n">datatype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;FP32&quot;</span>
<span class="n">shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="mi">1000</span><span class="p">]</span>
</pre></div>
</div>
</div><div aria-labelledby="tab-1-R1BV" class="sphinx-tabs-panel code-tab group-tab" hidden="true" id="panel-1-R1BV" name="R1BV" role="tabpanel" tabindex="0"><div class="highlight-text notranslate"><div class="highlight"><pre><span></span>name = &quot;resnet50&quot;
platform = &quot;onnx_onnxv1&quot;

[[inputs]]
name = &quot;input&quot;
datatype = &quot;FP32&quot;
shape = [224, 224, 3]

[[outputs]]
name = &quot;output&quot;
datatype = &quot;FP32&quot;
shape = [1000]
</pre></div>
</div>
</div><div aria-labelledby="tab-1-RlBHQQ==" class="sphinx-tabs-panel code-tab group-tab" hidden="true" id="panel-1-RlBHQQ==" name="RlBHQQ==" role="tabpanel" tabindex="0"><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">name = &quot;resnet50&quot;</span>
<span class="go">platform = &quot;vitis_xmodel&quot;</span>

<span class="go">[[inputs]]</span>
<span class="go">name = &quot;input&quot;</span>
<span class="go">datatype = &quot;INT8&quot;</span>
<span class="go">shape = [224, 224, 3]</span>

<span class="go">[[outputs]]</span>
<span class="go">name = &quot;output&quot;</span>
<span class="go">datatype = &quot;INT8&quot;</span>
<span class="go">shape = [1000]</span>
</pre></div>
</div>
</div></div>
<p>The name must match the name of the model directory: it defines the endpoint that will be used for inference.
The platform identifies the type of the model and determines the file extension of the model file.</p>
<p>The inputs and outputs define the list of input and output tensors for the model.
The names of the tensors may be significant if the backend needs them to perform inference.</p>
</section>
<section id="get-the-deployment-image">
<h2>Get the deployment image<a class="headerlink" href="#get-the-deployment-image" title="Permalink to this heading">¶</a></h2>
<p>The deployment image is optimized for size and only contains the run-time dependencies of the server to allow for quicker deployments.
It has limited debugging capabilities and it contains a precompiled executable for the server that automatically starts when the container starts.
You can pull the <a class="reference internal" href="deployment.html#deployment-image"><span class="std std-ref">deployment image</span></a> if it exists or build it yourself.</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-2-Q1BV" aria-selected="true" class="sphinx-tabs-tab code-tab group-tab" id="tab-2-Q1BV" name="Q1BV" role="tab" tabindex="0">CPU</button><button aria-controls="panel-2-R1BV" aria-selected="false" class="sphinx-tabs-tab code-tab group-tab" id="tab-2-R1BV" name="R1BV" role="tab" tabindex="-1">GPU</button><button aria-controls="panel-2-RlBHQQ==" aria-selected="false" class="sphinx-tabs-tab code-tab group-tab" id="tab-2-RlBHQQ==" name="RlBHQQ==" role="tab" tabindex="-1">FPGA</button></div><div aria-labelledby="tab-2-Q1BV" class="sphinx-tabs-panel code-tab group-tab" id="panel-2-Q1BV" name="Q1BV" role="tabpanel" tabindex="0"><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>pull<span class="w"> </span>amdih/serve:uif1.1_zendnn_amdinfer_0.3.0
</pre></div>
</div>
</div><div aria-labelledby="tab-2-R1BV" class="sphinx-tabs-panel code-tab group-tab" hidden="true" id="panel-2-R1BV" name="R1BV" role="tabpanel" tabindex="0"><div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ docker pull amdih/serve:uif1.1_migraphx_amdinfer_0.3.0
</pre></div>
</div>
</div><div aria-labelledby="tab-2-RlBHQQ==" class="sphinx-tabs-panel code-tab group-tab" hidden="true" id="panel-2-RlBHQQ==" name="RlBHQQ==" role="tabpanel" tabindex="0"><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp"># </span>this<span class="w"> </span>image<span class="w"> </span>is<span class="w"> </span>not<span class="w"> </span>currently<span class="w"> </span>pre-built<span class="w"> </span>but<span class="w"> </span>you<span class="w"> </span>can<span class="w"> </span>build<span class="w"> </span>it<span class="w"> </span>yourself
</pre></div>
</div>
</div></div>
</section>
<section id="start-the-image">
<h2>Start the image<a class="headerlink" href="#start-the-image" title="Permalink to this heading">¶</a></h2>
<p>You can start a container from the deployment image with <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span></code> as any other image.
For more information about these flags and other options you can use, refer to its <a class="reference external" href="https://docs.docker.com/engine/reference/commandline/run/">documentation</a>.
The flags used in this sample command are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--volume</span> <span class="pre">$(pwd)/model_repository:/mnt/models:rw</span></code>: mount the model repository you created above into the container to where the server expects it to be. This allows the server to load these models at startup.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--net=host</span></code>: use the host networking in the container. By default, the server uses ports 8998 and 50051 for HTTP and gRPC communication, respectively. By using this flag, you can use these same ports on the host to send requests to the server in the container.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--device</span> <span class="pre">&lt;path/to/device/file&gt;</span></code>: pass the specified files into the server. If you are using hardware accelerators, these devices need to be passed into the container so they can be used. For GPUs, this will be <code class="docutils literal notranslate"><span class="pre">--device</span> <span class="pre">/dev/kfd</span> <span class="pre">--device</span> <span class="pre">/dev/dri</span></code>. For FPGAs, this will be <code class="docutils literal notranslate"><span class="pre">--device</span> <span class="pre">/dev/dri</span> <span class="pre">--device</span> <span class="pre">/dev/xclmgmt&lt;id&gt;</span></code>, where the ID of the <code class="docutils literal notranslate"><span class="pre">xclmgmt</span></code> device will depend on your particular machine.</p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Using <code class="docutils literal notranslate"><span class="pre">--net=host</span></code> will not work if the default ports are unavailable on your host machine.
In this case, you can omit this flag and use <code class="docutils literal notranslate"><span class="pre">--publish</span></code> to map these ports in the container to other ports on your host machine.
Refer to the <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span></code> documentation for more information.</p>
</div>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-3-Q1BV" aria-selected="true" class="sphinx-tabs-tab code-tab group-tab" id="tab-3-Q1BV" name="Q1BV" role="tab" tabindex="0">CPU</button><button aria-controls="panel-3-R1BV" aria-selected="false" class="sphinx-tabs-tab code-tab group-tab" id="tab-3-R1BV" name="R1BV" role="tab" tabindex="-1">GPU</button><button aria-controls="panel-3-RlBHQQ==" aria-selected="false" class="sphinx-tabs-tab code-tab group-tab" id="tab-3-RlBHQQ==" name="RlBHQQ==" role="tab" tabindex="-1">FPGA</button></div><div aria-labelledby="tab-3-Q1BV" class="sphinx-tabs-panel code-tab group-tab" id="panel-3-Q1BV" name="Q1BV" role="tabpanel" tabindex="0"><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>run<span class="w"> </span>-d<span class="w"> </span>--volume<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>/model_repository:/mnt/models:rw<span class="w"> </span>--net<span class="o">=</span>host<span class="w"> </span>amdih/serve:uif1.1_zendnn_amdinfer_0.3.0
</pre></div>
</div>
</div><div aria-labelledby="tab-3-R1BV" class="sphinx-tabs-panel code-tab group-tab" hidden="true" id="panel-3-R1BV" name="R1BV" role="tabpanel" tabindex="0"><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>run<span class="w"> </span>-d<span class="w"> </span>--device<span class="w"> </span>/dev/kfd<span class="w"> </span>--device<span class="w"> </span>/dev/dri<span class="w"> </span>--volume<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>/model_repository:/mnt/models:rw<span class="w"> </span>--publish<span class="w"> </span><span class="m">127</span>.0.0.1::8998<span class="w"> </span>--publish<span class="w"> </span><span class="m">127</span>.0.0.1::50051<span class="w"> </span>amdih/serve:uif1.1_migraphx_amdinfer_0.3.0
</pre></div>
</div>
</div><div aria-labelledby="tab-3-RlBHQQ==" class="sphinx-tabs-panel code-tab group-tab" hidden="true" id="panel-3-RlBHQQ==" name="RlBHQQ==" role="tabpanel" tabindex="0"><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>run<span class="w"> </span>-d<span class="w"> </span>--device<span class="w"> </span>/dev/dri<span class="w"> </span>--device<span class="w"> </span>/dev/xclmgmt&lt;id&gt;<span class="w"> </span>--volume<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>/model_repository:/mnt/models:rw<span class="w"> </span>--publish<span class="w"> </span><span class="m">127</span>.0.0.1::8998<span class="w"> </span>--publish<span class="w"> </span><span class="m">127</span>.0.0.1::50051<span class="w"> </span>&lt;image&gt;
</pre></div>
</div>
</div></div>
<p>The endpoints for each model will be the name of the model in the <code class="docutils literal notranslate"><span class="pre">config.toml</span></code>, which should match the name of the parent directory in the model repository.
In this example, it would be “resnet50”.
You are now ready to make requests to this server.</p>
</section>
<section id="server-deployment-summary">
<span id="deploymentsummary"></span><h2>Server deployment summary<a class="headerlink" href="#server-deployment-summary" title="Permalink to this heading">¶</a></h2>
<p>After setting up the server as above, you have the following information:</p>
<ul class="simple">
<li><p>IP address: 127.0.0.1 since the server is running on the same machine where you will run the inference</p></li>
<li><p>Ports: 8998 and 50051 for HTTP and gRPC, respectively. If you used <code class="docutils literal notranslate"><span class="pre">--publish</span></code>, your port numbers may be different and you can see what they are using <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">ps</span></code>.</p></li>
<li><p>Endpoint: “resnet50” since that is what the model name was used in the model repository and in the configuration file</p></li>
</ul>
<p>The rest of this example will use these values in the sample code so substitute your own values if they are different.</p>
</section>
<section id="get-the-python-library">
<h2>Get the Python library<a class="headerlink" href="#get-the-python-library" title="Permalink to this heading">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">amdinfer</span></code> client library allows you to make clients that you can use to communicate with the server over any protocol that the server supports.
Clients for different protocols have the same base set of methods so you can easily replace one with another.
The library can be used from C++ or Python.
In this example, you will use the Python library, which you can install with <code class="docutils literal notranslate"><span class="pre">pip</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>pip<span class="w"> </span>install<span class="w"> </span>amdinfer
</pre></div>
</div>
</section>
<section id="running-an-example">
<h2>Running an example<a class="headerlink" href="#running-an-example" title="Permalink to this heading">¶</a></h2>
<p>The <a class="reference external" href="https://github.com/Xilinx/inference-server">AMD Inference Server repository</a> includes examples that demonstrate running an end-to-end inference request.
This particular example targets the ResNet50 model you’ve already deployed on the server above.
To perform the inference, you will need some files that are available in the repository:</p>
<pre class="literal-block">$ wget <a class="reference external" href="https://github.com/Xilinx/inference-server/raw/main/examples/resnet50/imagenet_classes.txt">https://github.com/Xilinx/inference-server/raw/main/examples/resnet50/imagenet_classes.txt</a>
$ wget <a class="reference external" href="https://github.com/Xilinx/inference-server/raw/main/tests/assets/dog-3619020_640.jpg">https://github.com/Xilinx/inference-server/raw/main/tests/assets/dog-3619020_640.jpg</a>
$ wget <a class="reference external" href="https://github.com/Xilinx/inference-server/raw/main/examples/resnet50/resnet.py">https://github.com/Xilinx/inference-server/raw/main/examples/resnet50/resnet.py</a></pre>
<p>These files provide the class labels, a sample image and a helper Python script.
You will also need the particular example file corresponding to the same hardware your server is intended for.
You can download this file and run it to perform an inference on the server.
At a high-level, the example script performs pre-processing on the image, constructs a request and sends it to the server.
The server responds with the results of the inference.
These results are post-processed and the top 5 labels for the image are printed.</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-4-Q1BV" aria-selected="true" class="sphinx-tabs-tab group-tab" id="tab-4-Q1BV" name="Q1BV" role="tab" tabindex="0">CPU</button><button aria-controls="panel-4-R1BV" aria-selected="false" class="sphinx-tabs-tab group-tab" id="tab-4-R1BV" name="R1BV" role="tab" tabindex="-1">GPU</button><button aria-controls="panel-4-RlBHQQ==" aria-selected="false" class="sphinx-tabs-tab group-tab" id="tab-4-RlBHQQ==" name="RlBHQQ==" role="tab" tabindex="-1">FPGA</button></div><div aria-labelledby="tab-4-Q1BV" class="sphinx-tabs-panel group-tab" id="panel-4-Q1BV" name="Q1BV" role="tabpanel" tabindex="0"><pre class="literal-block">$ wget <a class="reference external" href="https://github.com/Xilinx/inference-server/raw/main/examples/resnet50/tfzendnn.py">https://github.com/Xilinx/inference-server/raw/main/examples/resnet50/tfzendnn.py</a>
$ python3 tfzendnn.py --ip 127.0.0.1 --grpc-port 50051 --endpoint resnet50 --image ./dog-3619020_640.jpg --labels ./imagenet_classes.txt</pre>
</div><div aria-labelledby="tab-4-R1BV" class="sphinx-tabs-panel group-tab" hidden="true" id="panel-4-R1BV" name="R1BV" role="tabpanel" tabindex="0"><pre class="literal-block">$ wget <a class="reference external" href="https://github.com/Xilinx/inference-server/raw/main/examples/resnet50/migraphx.py">https://github.com/Xilinx/inference-server/raw/main/examples/resnet50/migraphx.py</a>
$ python3 migraphx.py --ip 127.0.0.1 --http-port 8998 --endpoint resnet50 --image ./dog-3619020_640.jpg --labels ./imagenet_classes.txt</pre>
</div><div aria-labelledby="tab-4-RlBHQQ==" class="sphinx-tabs-panel group-tab" hidden="true" id="panel-4-RlBHQQ==" name="RlBHQQ==" role="tabpanel" tabindex="0"><pre class="literal-block">$ wget <a class="reference external" href="https://github.com/Xilinx/inference-server/raw/main/examples/resnet50/vitis.py">https://github.com/Xilinx/inference-server/raw/main/examples/resnet50/vitis.py</a>
$ python3 vitis.py --ip 127.0.0.1 --http-port 8998 --endpoint resnet50 --image ./dog-3619020_640.jpg --labels ./imagenet_classes.txt</pre>
</div></div>
<p>After running the script, you should get output similar to the following.
The exact output may be slightly different depending on whether you used CPU, GPU or FPGA versions of the example.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Running the TF+ZenDNN example for ResNet50 in Python
Waiting until the server is ready...
Making inferences...
Top 5 classes for ./dog-3619020_640.jpg:
  n02112018 Pomeranian
  n02112350 keeshond
  n02086079 Pekinese, Pekingese, Peke
  n02112137 chow, chow chow
  n02113023 Pembroke, Pembroke Welsh corgi
</pre></div>
</div>
</section>
<section id="inference-summary">
<h2>Inference summary<a class="headerlink" href="#inference-summary" title="Permalink to this heading">¶</a></h2>
<p>To perform inference, you need to write an application that uses the AMD Inference Server’s client library to construct a request and send it to the server.
Communicating with the server requires you to know its address, IP and port, as well as an endpoint.
The endpoint uniquely identifies a particular model running in the server.
When there are multiple models running, the endpoint becomes important to differentiate where to send to send the request.
The server responds to the request and you can examine the results to take further action.</p>
</section>
<section id="next-steps">
<h2>Next steps<a class="headerlink" href="#next-steps" title="Permalink to this heading">¶</a></h2>
<p>To learn more about the Python script you ran to run the inference, walk through the script in this <a class="reference internal" href="example_resnet50_python.html#running-resnet50-python"><span class="std std-ref">example</span></a>.</p>
</section>
</section>


           </div>
          </div>
          
                  <style>
                        .footer {
                        position: fixed;
                        left: 0;
                        bottom: 0;
                        width: 100%;
                        }
                  </style>
				  
				  <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="terminology.html" class="btn btn-neutral float-left" title="Terminology" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="backends.html" class="btn btn-neutral float-right" title="Backends" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022 Advanced Micro Devices, Inc..
      <span class="lastupdated">Last updated on April 20, 2023.
      </span></p>
  </div>



										<div class="aem-Grid aem-Grid--16">
											<div class="aem-GridColumn aem-GridColumn--xxxlarge--none aem-GridColumn--xsmall--16 aem-GridColumn--offset--xsmall--0 aem-GridColumn--xlarge--none aem-GridColumn--xxlarge--none aem-GridColumn--default--none aem-GridColumn--offset--large--1 aem-GridColumn--xlarge--12 aem-GridColumn--offset--default--0 aem-GridColumn--xxlarge--10 aem-GridColumn--offset--xlarge--2 aem-GridColumn--offset--xxlarge--3 aem-GridColumn--offset--xxxlarge--4 aem-GridColumn--xsmall--none aem-GridColumn--large--none aem-GridColumn aem-GridColumn--large--14 aem-GridColumn--xxxlarge--8 aem-GridColumn--default--16">
												<div class="container-fluid sub-footer">

													                    <div class="row">
                        <div class="col-xs-24">
                          <p><a target="_blank" href="https://www.amd.com/en/corporate/copyright">Terms and Conditions</a> | <a target="_blank" href="https://www.amd.com/en/corporate/privacy">Privacy</a> | <a target="_blank" href="https://www.amd.com/en/corporate/cookies">Cookie Policy</a> | <a target="_blank" href="https://www.amd.com/en/corporate/trademarks">Trademarks</a> | <a target="_blank" href="https://www.amd.com/system/files/documents/statement-human-trafficking-forced-labor.pdf">Statement on Forced Labor</a> | <a target="_blank" href="https://www.amd.com/en/corporate/competition">Fair and Open Competition</a> | <a target="_blank" href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf">UK Tax Strategy</a> | <a target="_blank" href="https://docs.xilinx.com/v/u/9x6YvZKuWyhJId7y7RQQKA">Inclusive Terminology</a> | <a href="#cookiessettings" class="ot-sdk-show-settings">Cookies Settings</a></p>
                        </div>
                    </div>
												</div>
											</div>
										</div>
										
</br>


  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  

  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      <span class="fa fa-book"> Read the Docs</span>
      v: main
      <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
      
      <dl>
        <dt>Languages</dt>
        
           <strong> 
          <dd><a href="/inference-server/main/">en</a></dd>
           </strong> 
        
      </dl>
      
      
      <dl>
        <dt>Versions</dt>
        
          
          <dd><a href="/inference-server/0.1.0/">0.1.0</a></dd>
          
        
          
          <dd><a href="/inference-server/0.2.0/">0.2.0</a></dd>
          
        
          
          <dd><a href="/inference-server/0.3.0/">0.3.0</a></dd>
          
        
           <strong> 
          <dd><a href="/inference-server/main/">main</a></dd>
           </strong> 
        
      </dl>
      
      
       
    </div>
  </div>
<script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(false);
      });
  </script>
 <script type="text/javascript">
    $(document).ready(function() {
        $(".toggle > *").hide();
        $(".toggle .header").show();
        $(".toggle .header").click(function() {
            $(this).parent().children().not(".header").toggle(400);
            $(this).parent().children(".header").toggleClass("open");
        })
    });
</script>


</body>
</html>