
.. _program_listing_file__workspace_amdinfer_include_amdinfer_core_inference_response.hpp:

Program Listing for File inference_response.hpp
===============================================

|exhale_lsh| :ref:`Return to documentation for file <file__workspace_amdinfer_include_amdinfer_core_inference_response.hpp>` (``/workspace/amdinfer/include/amdinfer/core/inference_response.hpp``)

.. |exhale_lsh| unicode:: U+021B0 .. UPWARDS ARROW WITH TIP LEFTWARDS

.. code-block:: cpp

   // Copyright 2023 Advanced Micro Devices, Inc.
   //
   // Licensed under the Apache License, Version 2.0 (the "License");
   // you may not use this file except in compliance with the License.
   // You may obtain a copy of the License at
   //
   //      http://www.apache.org/licenses/LICENSE-2.0
   //
   // Unless required by applicable law or agreed to in writing, software
   // distributed under the License is distributed on an "AS IS" BASIS,
   // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   // See the License for the specific language governing permissions and
   // limitations under the License.
   
   #ifndef GUARD_AMDINFER_CORE_INFERENCE_RESPONSE
   #define GUARD_AMDINFER_CORE_INFERENCE_RESPONSE
   
   #include "amdinfer/build_options.hpp"          // for AMDINFER_ENABLE_TRACING
   #include "amdinfer/core/inference_tensor.hpp"  // for InferenceTensor
   #include "amdinfer/core/parameters.hpp"        // for ParameterMap
   #include "amdinfer/declarations.hpp"           // for InferenceResponseOutput
   
   namespace amdinfer {
   
   class InferenceResponseOutput : public InferenceTensor {
    public:
     InferenceResponseOutput();
   
     // /**
     //  * @brief Construct a new InferenceResponseOutput object
     //  *
     //  * @param data pointer to data
     //  * @param shape shape of the data
     //  * @param data_type type of the data
     //  * @param name name to assign
     //  */
     // InferenceResponseOutput(void *data, std::vector<uint64_t> shape,
     //                       DataType data_type, std::string name = "");
   
     void setData(std::vector<std::byte> &&buffer);
     [[nodiscard]] void *getData() const;
   
     [[nodiscard]] size_t serializeSize() const override;
     std::byte *serialize(std::byte *data_out) const override;
     const std::byte *deserialize(const std::byte *data_in) override;
   
     friend std::ostream &operator<<(std::ostream &os,
                                     InferenceResponseOutput const &my_class);
   
    private:
     std::vector<std::byte> data_;
   };
   
   class InferenceResponse {
    public:
     InferenceResponse();
   
     explicit InferenceResponse(const std::string &error);
   
     [[nodiscard]] std::vector<InferenceResponseOutput> getOutputs() const;
     void addOutput(const InferenceResponseOutput &output);
   
     std::string getID() const { return id_; }
     void setID(const std::string &id);
     void setModel(const std::string &model);
     std::string getModel();
   
     bool isError() const;
     std::string getError() const;
   
   #ifdef AMDINFER_ENABLE_TRACING
   
     void setContext(StringMap &&context);
     const StringMap &getContext() const;
   #endif
   
     ParameterMap *getParameters() { return this->parameters_.get(); }
   
     friend std::ostream &operator<<(std::ostream &os,
                                     InferenceResponse const &my_class);
   
    private:
     std::string model_;
     std::string id_;
     std::shared_ptr<ParameterMap> parameters_;
     std::vector<InferenceResponseOutput> outputs_;
     std::string error_msg_;
   #ifdef AMDINFER_ENABLE_TRACING
     StringMap context_;
   #endif
   };
   
   }  // namespace amdinfer
   
   #endif  // GUARD_AMDINFER_CORE_INFERENCE_RESPONSE
