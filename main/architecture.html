


<!DOCTYPE HTML>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
	<head>
		<meta charset="utf-8">
		<meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

		<meta http-equiv="content-type" content="text/html; charset=UTF-8"/>
		<link rel="stylesheet" href="https://static.cloud.coveo.com/searchui/v2.4382/css/CoveoFullSearch.css"/>
		<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
		<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
		<meta name="description"/>
		<meta name="keywords"/>
		<meta property="og:title" content=""/>
		<meta property="og:description"/>
		<!-- favicon -->
		<link rel="icon" type="image/vnd.microsoft.icon" href="_static/favicon.ico"/>
		<link rel="shortcut icon" type="image/vnd.microsoft.icon" href="_static/favicon.ico"/>
		<!-- Fonts -->
		<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,500" rel="stylesheet" type="text/css"/>

	<!-- OneTrust Cookies Consent Notice start for xilinx.github.io -->
<script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-document-language="true" type="text/javascript" charset="UTF-8" data-domain-script="37701bd4-e5c0-4ee3-9329-e7475d0b13a7" ></script>
<script type="text/javascript">
function OptanonWrapper() { }
</script>
<!-- OneTrust Cookies Consent Notice end for xilinx.github.io -->
		<!-- Google Tag Manager -->
	<script type="text/plain" class="optanon-category-C0002">(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
	new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
	j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
	'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
	})(window,document,'script','dataLayer','GTM-5RHQV7');</script>
	<!-- End Google Tag Manager -->
	
        <!-- Google Tag Manager -->
        <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-5RHQV7" height="0" width="0" style="display:none;visibility:hidden" class="optanon-category-C0002"></iframe></noscript>
        <!-- End Google Tag Manager -->	

  
  
  
  

  
      <script type="text/javascript" src="_static/js/jquery.min.js"></script>
	  <script type="text/javascript" src="_static/js/gtm.js"></script>
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
    <script type="text/javascript" src="_static/js/d3dd8c60ed.js"></script>
    <script type="text/javascript" src="_static/js/common-ui-all.min.js"></script>
    <script type="text/javascript" src="_static/js/jquery-ui.min.js"></script>
    <script type="text/javascript" src="_static/js/CoveoJsSearch.Lazy.min.js"></script>
    <script type="text/javascript" src="_static/js/linkid.js"></script>
    <script type="text/javascript" src="_static/js/Searchbox.min.js"></script>
    <script type="text/javascript" src="_static/js/header-footer.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/common-ui-all.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/header-footer.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/pro.min.css" media="all" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Contributing" href="contributing.html" />
    <link rel="prev" title="Python API" href="python_api.html" /> 
	</head>
	<body>
		<div class="xilinx-bs3"/>
		<div class="root responsivegrid">
			<div class="aem-Grid aem-Grid--16 aem-Grid--default--16 aem-Grid--large--16 aem-Grid--xlarge--16 aem-Grid--xxlarge--16 aem-Grid--xxxlarge--16 ">
 <div class="xilinxExperienceFragments experiencefragment aem-GridColumn aem-GridColumn--default--12">

    
    

    



<div class="xf-content-height">
    

<div class="aem-Grid aem-Grid--16 aem-Grid--default--16 ">
    
    <div class="header parbase aem-GridColumn aem-GridColumn--default--12"><noindex>
	<header data-component="header" class="site-header">
		<nav class="navbar navbar-default">
			<div class="container-fluid main-nav">
				<div class="row">
					<div class="navbar-column col-xs-4 col-sm-5">
						<ul class="nav navbar-nav hidden-xs">
							<li>
									<button onclick="window.location.href=&#39;https://www.xilinx.com/applications.html&#39;;">Solutions</button>
								</li>
							<li>
									<button onclick="window.location.href=&#39;https://www.xilinx.com/products/silicon-devices.html&#39;;">Products</button>
								</li>
							<li>
									<button onclick="window.location.href=&#39;https://www.xilinx.com/about/company-overview.html&#39;;">Company</button>
								</li>
							</ul>
							
						<ul class="nav navbar-nav hidden-sm hidden-md hidden-lg">
							<li>
									<button data-target="#header-container-0" data-function="toggle">Solutions</button>
								</li>
							<li>
									<button data-target="#header-container-1" data-function="toggle">Products</button>
								</li>
							<li>
									<button data-target="#header-container-2" data-function="toggle">Company</button>
								</li>
							</ul>
							
						<div id="header-container-0" class="menu-container">
								<div class="navbar-nav-container">
									<ul class="nav navbar-nav">
										<li>
												<button onclick="window.location.href=&#39;https://www.xilinx.com/applications.html&#39;;">Solutions</button>
												</li>
										<li>
												<button onclick="window.location.href=&#39;https://www.xilinx.com/products/silicon-devices.html&#39;;">Products</button>
												</li>
										<li>
												<button onclick="window.location.href=&#39;https://www.xilinx.com/about/company-overview.html&#39;;">Company</button>
												</li>
										</ul>
									<button data-function="close-menu">
										<span class="fal fa-times" aria-hidden="true"></span>
									</button>
								</div>
							</div>
							<div id="header-container-1" class="menu-container">
								<div class="navbar-nav-container">
									<ul class="nav navbar-nav">
										<li>
												<button onclick="window.location.href=&#39;https://www.xilinx.com/applications.html&#39;;">Solutions</button>
												</li>
										<li>
												<button onclick="window.location.href=&#39;https://www.xilinx.com/products/silicon-devices.html&#39;;">Products</button>
												</li>
										<li>
												<button onclick="window.location.href=&#39;https://www.xilinx.com/about/company-overview.html&#39;;">Company</button>
												</li>
										</ul>
									<button data-function="close-menu">
										<span class="fal fa-times" aria-hidden="true"></span>
									</button>
								</div>
							</div>
							<div id="header-container-2" class="menu-container">
								<div class="navbar-nav-container">
									<ul class="nav navbar-nav">
										<li>
												<button onclick="window.location.href=&#39;https://www.xilinx.com/applications.html&#39;;">Solutions</button>
												</li>
										<li>
												<button onclick="window.location.href=&#39;https://www.xilinx.com/products/silicon-devices.html&#39;;">Products</button>
												</li>
										<li>
												<button onclick="window.location.href=&#39;https://www.xilinx.com/about/company-overview.html&#39;;">Company</button>
												</li>
										</ul>
									<button data-function="close-menu">
										<span class="fal fa-times" aria-hidden="true"></span>
									</button>
								</div>
							</div>
							</div>
					<div class="logo-column col-xs-4 col-sm-2">
						<div class="logo">
							<a target="_blank" href="https://www.xilinx.com/">
								<img src="https://github.com/Xilinx/Image-Collateral/blob/main/xilinx-header-logo.svg?raw=true" title="Xilinx Inc"/>
							</a>
						</div>
					</div>

				</div>
			</div>
			</nav></header></noindex></div>
		
	
</div>

    
</div>
</div>

<div class="calloutBanner parbase aem-GridColumn--default--none aem-GridColumn aem-GridColumn--offset--default--0 aem-GridColumn--default--16"><div class="callout-banner">
    Xilinx is now a part of <a target="_blank" href="https://www.amd.com/en/corporate/xilinx-acquisition">AMD</a> |  <a target="_blank" href="https://www.amd.com/en/corporate/privacy">Learn More</a>
</div>
</div>
				<div class="parsys aem-GridColumn--xxxlarge--none aem-GridColumn--xlarge--none aem-GridColumn--xxlarge--none aem-GridColumn--default--none aem-GridColumn--offset--large--1 aem-GridColumn--xlarge--12 aem-GridColumn--offset--default--0 aem-GridColumn--xxlarge--10 aem-GridColumn--offset--xlarge--2 aem-GridColumn--offset--xxlarge--3 aem-GridColumn--offset--xxxlarge--4 aem-GridColumn--large--none aem-GridColumn aem-GridColumn--large--14 aem-GridColumn--xxxlarge--8 aem-GridColumn--default--16">
						<div class="container-fluid">
							<div class="row">
							<div class="col-xs-12">
   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> AMD Inference Server
          

          
          </a>

          
            
            
              <div class="version">
                main
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

      
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
            
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="index.html">AMD Inference Server</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="hello_world_python.html">Hello World (Python)</a></li>
<li class="toctree-l1"><a class="reference internal" href="xmodel_example_python.html">Running a Vitis AI XModel (Python)</a></li>
<li class="toctree-l1"><a class="reference internal" href="xmodel_example_cpp.html">Running a Vitis AI XModel (C++)</a></li>
<li class="toctree-l1"><a class="reference internal" href="zendnn_examples.html">ZenDNN Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">Command-Line Interface</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Getting Performance</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="performance_factors.html">Factors</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Using AMD Inference Server</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="aks.html">AKS</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">Logs</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmarking.html">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="metrics.html">Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="tracing.html">Tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="kserve.html">KServe</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Platforms</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="vitis_ai.html">Vitis AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="zendnn.html">ZenDNN</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User API libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="rest.html">REST Endpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_api.html">C++ API</a></li>
<li class="toctree-l1"><a class="reference internal" href="python_api.html">Python API</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developers</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Architecture</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#ingestion">Ingestion</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#api">API</a></li>
<li class="toctree-l3"><a class="reference internal" href="#http-rest-and-websocket">HTTP/REST and WebSocket</a></li>
<li class="toctree-l3"><a class="reference internal" href="#c-api">C++ API</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#batching">Batching</a></li>
<li class="toctree-l2"><a class="reference internal" href="#workers">Workers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#organization-and-lifecycle">Organization and Lifecycle</a></li>
<li class="toctree-l3"><a class="reference internal" href="#improving-performance">Improving Performance</a></li>
<li class="toctree-l3"><a class="reference internal" href="#external-processing">External Processing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#xmodel">XModel</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#initialization">Initialization</a></li>
<li class="toctree-l4"><a class="reference internal" href="#allocation">Allocation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#acquisition">Acquisition</a></li>
<li class="toctree-l4"><a class="reference internal" href="#run">Run</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cleanup">Cleanup</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#buffering">Buffering</a></li>
<li class="toctree-l2"><a class="reference internal" href="#manager">Manager</a></li>
<li class="toctree-l2"><a class="reference internal" href="#observation">Observation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#logging">Logging</a></li>
<li class="toctree-l3"><a class="reference internal" href="#metrics">Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tracing">Tracing</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">About</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="roadmap.html">Roadmap</a></li>
<li class="toctree-l1"><a class="reference internal" href="changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="dependencies.html">Dependencies</a></li>
</ul>

            
			
			<p class="caption"><span class="caption-text">This Page</span></p>
				<ul class="current">
				  <li class="toctree-l1"><a href="_sources/architecture.rst.txt"
						rel="nofollow">Show Source</a></li>
				</ul>
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">AMD Inference Server</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Architecture</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/architecture.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="architecture">
<h1>Architecture<a class="headerlink" href="#architecture" title="Permalink to this heading">¶</a></h1>
<figure class="align-left" id="id4">
<span id="architecture-overview"></span><a class="reference internal image-reference" href="_images/architecture.png"><img alt="Diagram showing AMD Inference Server's architecture" src="_images/architecture.png" style="height: 400px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 1 </span><span class="caption-text">Server architecture overview</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
<div class="legend">
</div>
</figcaption>
</figure>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this heading">¶</a></h2>
<p><a class="reference internal" href="#architecture-overview"><span class="std std-numref">Fig. 1</span></a> shows a high-level view of the Server’s architecture.
At the top, we have a client who can make a request to the Server using HTTP/REST, WebSocket, or the C++ API <a class="footnote-reference brackets" href="#f1" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>.
This request must be made to a particular worker that must be active in the Server.
Each received request is passed to a batcher associated with the targeted worker independent of its origin.
The batcher will combine individual requests into a batch and pass it to the worker for execution.
The worker parses the batch, processes each request contained within, and responds to the client.
Internally, the worker can leverage any C++ logic and/or use external libraries such as Vitis AI or machine learning frameworks to process the request.
This flexibility enables the Server to take advantage of any hardware accelerator with the appropriate worker.
The response to the client is made using the same API as the original request.</p>
</section>
<section id="ingestion">
<h2>Ingestion<a class="headerlink" href="#ingestion" title="Permalink to this heading">¶</a></h2>
<p>There are a number of ways to get data into the system for inference.
In general, each protocol has a custom interface to the client and requires explicit handling in the Server to accept this initial request.
After receiving the client’s request in the Server, each supported protocols’ handler packs the request into an implementation of an <code class="docutils literal notranslate"><span class="pre">Interface</span></code> object.
This virtual class is defined in the Server and provides the batcher with a standard way of interacting with data from different ingestion protocols.
Since all protocols push an implementation of this class to the batcher, they are treated equally by the rest of the Server.
The only protocol-specific logic applies at the end of the worker when it replies back to the client in a protocol-specific manner.
This structure also enables easy extension of the Server to add new protocols by extending the <code class="docutils literal notranslate"><span class="pre">Interface</span></code> class.</p>
<section id="api">
<h3>API<a class="headerlink" href="#api" title="Permalink to this heading">¶</a></h3>
<p>The APIs used for this project are based on ‘<a class="reference external" href="https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md">KServe’s v2 specification</a>’ where possible.
The API defines endpoints in different categories: health, metrics and prediction.
The health APIs define endpoints to get Server metadata and check server readiness.
The metrics API defines a single endpoint for exposing the collected metrics using the Prometheus format.
The prediction APIs are used to make inferences.</p>
<p>For inference, we provide <code class="docutils literal notranslate"><span class="pre">load</span></code> and <code class="docutils literal notranslate"><span class="pre">unload</span></code> endpoints for clients to control which workers are active (and how many instances of each).
The load API accepts an optional set of parameters that define <strong>load-time parameters</strong>.
On success, the load API returns a string corresponding to the endpoint that clients should use to make prediction requests from the loaded worker.
Note, this usage differs slightly from KServe’s specifications.
With KServe, the load API returns only the status code and the requests are made to the same endpoint as the string specified in the load request.
It also doesn’t have the notion of load-time parameters.
In our case, there are parameters we need to pass at load-time, which results in potentially different endpoints if multiple workers with different configurations are loaded at once.
To maintain compatibility, we do guarantee that the first worker loaded for a particular model, independent of configuration, will be at the same endpoint as the load request.
Therefore, a KServe client is free to ignore the contents of the response and make requests to the endpoint without resulting in errors.</p>
<p>We currently do not support the optional version information associated with a model defined in the KServe specification.
After a particular worker is loaded, inference requests can be made to it by constructing the appropriate request object and sending it to the prediction endpoint.
The format of the request object in HTTP matches KServe’s specification while an equivalent C++ object is used for the C++ API.</p>
</section>
<section id="http-rest-and-websocket">
<h3>HTTP/REST and WebSocket<a class="headerlink" href="#http-rest-and-websocket" title="Permalink to this heading">¶</a></h3>
<p>The HTTP/REST and WebSocket functionality in the Server is provided using <a class="reference external" href="https://github.com/drogonframework/drogon">Drogon</a>. We chose to use Drogon for our web framework for a few reasons:</p>
<ul class="simple">
<li><p>Based on the benchmarks at ‘<a class="reference external" href="https://github.com/TechEmpower/FrameworkBenchmarks/">&lt;TechEmpower</a>’, Drogon is high-performing (unlike CppCMS and Treefrog)</p></li>
<li><p>It is more stable and active than Lithium, another high-performing framework (Lithium is newer)</p></li>
<li><p>Active on Github with versioned releases (unlike Pistache and Lithium)</p></li>
</ul>
<p>The various endpoints from the API are registered in the Drogon’s HTTP controller along with their request handler functions.
Drogon uses a configurable number of threads to run these request handlers.
When a REST request is made to an endpoint, the request data and callback function are provided for the handler to process the request and then respond to the client.
To avoid blocking the finite number of handler threads with potentially long-running inference requests, we use an asynchronous architecture in the handler.
The received request is packed into an <code class="docutils literal notranslate"><span class="pre">Interface</span></code> object and pushed into a <a class="reference external" href="https://github.com/cameron314/concurrentqueue">thread-safe lock-free multi producer/consumer queue</a> to go to the target worker’s batcher.
The HTTP server code is in <code class="docutils literal notranslate"><span class="pre">src/proteus/servers/http_server.*</span></code>.</p>
<p>Drogon also provides a WebSocket server, which is currently used experimentally to run predictions on videos from certain workers.
The WebSocket API is custom.
At this time, the client provides a URL to a video that the worker will retrieve and analyze frame-by-frame and send back to the client but this is subject to change.
The WebSocket server code is in <code class="docutils literal notranslate"><span class="pre">src/proteus/servers/websocket_server.*</span></code>.</p>
</section>
<section id="c-api">
<h3>C++ API<a class="headerlink" href="#c-api" title="Permalink to this heading">¶</a></h3>
<p>The C++ API allows users to compile custom applications that link directly to the Server’s backend.
As a result, using the C++ API will yield the highest performance of any ingestion method.</p>
<p>The C++ API provides functions similar to the prediction API used in HTTP.
The API lets users load workers and make inference requests.
The inference request is packed into an <code class="docutils literal notranslate"><span class="pre">Interface</span></code> object and pushed to the target worker’s batcher.
An <code class="docutils literal notranslate"><span class="pre">std::promise</span></code> is returned to the user to retrieve the result.</p>
<p>The public API is defined in <code class="docutils literal notranslate"><span class="pre">include/proteus/clients/native.hpp</span></code> and the implementation is in <code class="docutils literal notranslate"><span class="pre">src/proteus/clients/native.cpp</span></code>.</p>
</section>
</section>
<section id="batching">
<h2>Batching<a class="headerlink" href="#batching" title="Permalink to this heading">¶</a></h2>
<figure class="align-left" id="id5">
<span id="architecture-detail"></span><a class="reference internal image-reference" href="_images/architecture_detailed.png"><img alt="Diagram showing more detail in to the AMD Inference Server's architecture" src="_images/architecture_detailed.png" style="height: 400px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2 </span><span class="caption-text">More detailed look at the Server architecture</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
<div class="legend">
</div>
</figcaption>
</figure>
<p>Batching is a technique used in hardware to improve throughput performance.
Batching groups multiple smaller requests from the user into one large request to improve the performance of hardware accelerators.
However, user requests at the software application level are usually not conveniently available as complete batches as they come one at a time.
The Server incorporates batching as a transparent step in the pipeline that groups all incoming requests, independent of the source of the original request from the client (see <a class="reference internal" href="#architecture-detail"><span class="std std-numref">Fig. 2</span></a>).
The implementations of the batchers are in <code class="docutils literal notranslate"><span class="pre">src/proteus/batching</span></code>.</p>
<p>The base batcher class defines a common interface for all batcher implementations and has some basic common properties.
Each batcher has two thread-safe queues (one for input and one for output), a configured batch size and a string identifying the worker group it’s attached to.
The batcher runs as a separate thread that monitors its input queue to process incoming <code class="docutils literal notranslate"><span class="pre">Interface</span></code> objects from all ingestion methods and pushes completed <code class="docutils literal notranslate"><span class="pre">Batch</span></code> objects on the output queue.
Each batcher implementation defines a <code class="docutils literal notranslate"><span class="pre">run()</span></code> method that provides the logic with which the batcher produces a batch.
A worker (and by extension, the worker group) specifies which batcher implementation should be used to prepare batches for it (as well as the batch size) and each worker group shares a set of batchers.
This configuration is determined at compile-time and built into the definition of the worker.</p>
<p>A <code class="docutils literal notranslate"><span class="pre">Batch</span></code> is made up of three basic components: <code class="docutils literal notranslate"><span class="pre">InferenceRequest</span></code> objects and input/output buffers.
<code class="docutils literal notranslate"><span class="pre">InferenceRequest</span></code> objects are direct C++ implementations of the information present in the KServe API of an inference request structured in a similar format.
They provide an ingestion-agnostic data format that all workers can process.
The worker group that the batcher is attached to provides a set of input/output buffers from a pool of buffers that it allocates.
These buffers are available in a queue for that batcher to pull from when it has incoming requests to batch together.
Most commonly, each buffer can be used to represent one batch-size worth of contiguous memory but its exact nature depends on the buffer implementation that the worker is using.
In this case, the batcher’s job is to take individual requests and move its data into one slot of this buffer and construct the corresponding <code class="docutils literal notranslate"><span class="pre">InferenceRequest</span></code> object.
Batchers have some flexibility with how these batches are constructed, which is why multiple batcher implementations are possible and supported in the AMD Inference Server.
For example, one batcher may allow partial batches to be pushed on after enough time whereas this may not be allowed by another batcher.</p>
<p>Batchers use the <code class="docutils literal notranslate"><span class="pre">Interface</span></code> object’s <code class="docutils literal notranslate"><span class="pre">getRequest()</span></code> method to help create batches.
This method must be implemented by each interface and governs how, given some buffers and counters, the particular ingestion method’s data should be converted to an <code class="docutils literal notranslate"><span class="pre">InferenceRequest</span></code> and its data is copied over to the buffers.
THis method allows batchers to process all ingestion methods without knowing about the details of how the data may be stored internally in the <code class="docutils literal notranslate"><span class="pre">Interface</span></code>.</p>
</section>
<section id="workers">
<span id="architectureworkers"></span><h2>Workers<a class="headerlink" href="#workers" title="Permalink to this heading">¶</a></h2>
<p>Workers perform the computation.
They are the smallest unit that the Server manages.
A worker may be as simple or complex as you like: as long as it adheres to the interface.
Each worker is compiled as a shared object that the Server can dynamically open at load-time.
Thus, new workers can be loaded and unloaded without stopping the server.</p>
<p>Workers are defined in <code class="docutils literal notranslate"><span class="pre">src/proteus/workers</span></code>.
The <code class="docutils literal notranslate"><span class="pre">CMakeLists.txt</span></code> file builds each worker as <code class="docutils literal notranslate"><span class="pre">libworkerX.so</span></code> where <em>X</em> corresponds to the name of the C++ file defining the worker in PascalCase.</p>
<section id="organization-and-lifecycle">
<h3>Organization and Lifecycle<a class="headerlink" href="#organization-and-lifecycle" title="Permalink to this heading">¶</a></h3>
<p>The base Worker class provides the template of all workers for the Server.
This class defines the lifecycle methods of the worker that are called by the Server.
This lifecycle is defined as follows:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">init()</span></code>: perform low-cost initialization of the worker</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">allocate()</span></code>: allocate memory buffers that are used to hold input and output data for the worker. <cite>Buffering</cite> is further discussed below.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">acquire()</span></code>: acquire any hardware accelerators/resources and/or perform any high-cost initialization for the worker</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">run()</span></code>: the main body of the worker performs the chosen computations on incoming batches</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">release()</span></code>: release any hardware accelerators/resources</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">deallocate()</span></code>: free the memory buffers allocated by this worker</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">destroy()</span></code>: perform any final operations prior to shutdown</p></li>
</ol>
<p>The first three steps set up the worker while the latter three tear it down and are performed in this order by the Server.
The body of these methods must be provided by each worker implementation in the corresponding <code class="docutils literal notranslate"><span class="pre">doX()</span></code> methods (e.g. <code class="docutils literal notranslate"><span class="pre">doInit()</span></code>).
At load-time, the server will create an instance of the worker using its <code class="docutils literal notranslate"><span class="pre">getWorker()</span></code> method:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">extern</span><span class="w"> </span><span class="s">&quot;C&quot;</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">proteus</span><span class="o">::</span><span class="n">workers</span><span class="o">::</span><span class="n">Worker</span><span class="o">*</span><span class="w"> </span><span class="nf">getWorker</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">proteus</span><span class="o">::</span><span class="n">workers</span><span class="o">::</span><span class="n">MyWorkerClass</span><span class="p">();</span><span class="w"> </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<p>This instance is saved internally and the first three methods above are called to initialize the worker.
The worker’s batcher is also started by the server at this time.
Finally, the worker’s <code class="docutils literal notranslate"><span class="pre">run()</span></code> method is started as a separate thread with the batcher’s output queue passed as the input queue to the worker.
This method performs the body of the work.
In an infinite loop, this method should wait for incoming batches from the worker’s input queue, process the requests, and respond to the clients.</p>
<p>To unload a worker, the Manager sends a <code class="docutils literal notranslate"><span class="pre">nullptr</span></code> to the worker, which should terminate the <code class="docutils literal notranslate"><span class="pre">run()</span></code> thread.
This thread is joined and the last three lifecycle methods are called to safely clean up the worker.</p>
</section>
<section id="improving-performance">
<h3>Improving Performance<a class="headerlink" href="#improving-performance" title="Permalink to this heading">¶</a></h3>
<p>Having multiple workers of the same kind can improve performance if there are many incoming batches.
Multiple identical workers are grouped into one worker group (see <a class="reference internal" href="#architecture-detail"><span class="std std-numref">Fig. 2</span></a>).
Each worker group shares one batcher group i.e. each batcher in a batcher group pushes its batches to a common queue that each worker in a worker group is dequeuing from.
This structure enables any worker in the group to pull a new batch when it can process it.
Therefore, each worker should only pull from this common queue when it can actually process the data.
To load a new worker into an existing group, the worker should be loaded with the load-time parameter <code class="docutils literal notranslate"><span class="pre">share</span></code> set to <em>false</em>.</p>
</section>
<section id="external-processing">
<h3>External Processing<a class="headerlink" href="#external-processing" title="Permalink to this heading">¶</a></h3>
<p>Workers, by virtue of their generic structure, may be highly complex and call entirely external applications for processing data.
The AMD Inference Server supports this use case and suggests the following for organizing code:</p>
<ul class="simple">
<li><p>The external application can be brought in similarly to how existing external applications are brought in already with CMake</p></li>
<li><p>The general worker structure should follow the existing model for native workers as defined above</p></li>
<li><p>After determining that a request is valid, the worker should convert the native request into something that the external application understands</p></li>
<li><p>Then, the data can be passed over to the external application.</p></li>
<li><p>The external application should return its results back to the worker</p></li>
<li><p>The response needs to be converted back to the native format to reply to the client</p></li>
</ul>
<p>Currently, there are no rules that the Server enforces for what workers are allowed to do and if they must expose any other functionality to the Server though this will change in the future.
For example, the Server will eventually need to send health check requests to workers that must be responded to appropriately.</p>
</section>
<section id="xmodel">
<h3>XModel<a class="headerlink" href="#xmodel" title="Permalink to this heading">¶</a></h3>
<figure class="align-left" id="id6">
<span id="fig-xmodel"></span><a class="reference internal image-reference" href="_images/xmodel.png"><img alt="Diagram showing the structure of the XModel worker" src="_images/xmodel.png" style="height: 300px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 3 </span><span class="caption-text">The XModel worker</span><a class="headerlink" href="#id6" title="Permalink to this image">¶</a></p>
<div class="legend">
</div>
</figcaption>
</figure>
<p>As perhaps the most complex worker thus far, the architecture of the XModel worker is examined here in greater detail.
The XModel worker is intended to run an arbitrary XModel specified by the user on a Xilinx FPGA <a class="footnote-reference brackets" href="#f3" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>.
We take a look at the lifecycle of this worker in the following sections.</p>
<section id="initialization">
<h4>Initialization<a class="headerlink" href="#initialization" title="Permalink to this heading">¶</a></h4>
<p>The XModel worker needs a path to an XModel to run at load-time.
This XModel file is opened and parsed to get the graph and the first DPU subgraph (i.e. the first subgraph in the graph that is supposed to run on the FPGA).
In the future, we may support running an arbitrary number of subgraphs but this simple case is often sufficient.
Using this subgraph, we create a <em>Runner</em>, which is a thread-safe object defined in the Vitis-AI runtime and is responsible for submitting requests to the FPGA.
These objects are all saved as part of the internal state of the worker.</p>
</section>
<section id="allocation">
<h4>Allocation<a class="headerlink" href="#allocation" title="Permalink to this heading">¶</a></h4>
<p>We use a special buffer backend for the this worker: the VartTensorBuffer.
This custom type provides better compatibility with using the Runner as that expects <code class="docutils literal notranslate"><span class="pre">vart::TensorBuffer</span></code> objects to pass data to the FPGA.
Therefore, this worker creates buffers using this backend and passes them to the Manager.</p>
</section>
<section id="acquisition">
<h4>Acquisition<a class="headerlink" href="#acquisition" title="Permalink to this heading">¶</a></h4>
<p>Since the Runner is thread-safe, we can use multiple threads to push data to the FPGA from the same Runner to improve throughput.
To enable this functionality, we incorporate an internal thread pool in the XModel worker.
Here, we set the size of this thread pool based on user parameters.</p>
</section>
<section id="run">
<h4>Run<a class="headerlink" href="#run" title="Permalink to this heading">¶</a></h4>
<p>As with all workers, the XModel worker pulls batches from its inputs queue and checks if it’s a <code class="docutils literal notranslate"><span class="pre">nullptr</span></code> before continuing to process the batch.
If valid, the batch is pushed into the thread pool, which internally assigns a lambda function to one of its internal threads to perform the processing.
This lambda function performs the same work that other workers normally perform directly in the <code class="docutils literal notranslate"><span class="pre">run()</span></code> method itself.
Here, for each batch, we push the data to the FPGA with the Runner and start preparing the response while waiting for the asynchronous operation to return.
Then, the response from the FPGA is parsed, the client response is populated with this data and the callback is called to respond back to the client.</p>
<p>To prevent the worker from pulling too many batches, an atomic counter is used to track the number of outstanding batches in the worker.
If the number is above a configured amount, then the worker doesn’t pull more batches until it has processed some of the ones it already has.
This throttling is necessary for the work-stealing model for workers to work.</p>
</section>
<section id="cleanup">
<h4>Cleanup<a class="headerlink" href="#cleanup" title="Permalink to this heading">¶</a></h4>
<p>There is almost no special cleanup required as the Vitis-AI objects that are part of the worker’s state are smart pointers and are cleaned by the worker’s destructor.
THe only non-default implementation of the clean-up functions is to stop the internal thread pool and join the threads.</p>
</section>
</section>
</section>
<section id="buffering">
<h2>Buffering<a class="headerlink" href="#buffering" title="Permalink to this heading">¶</a></h2>
<figure class="align-left" id="id7">
<span id="fig-buffering"></span><a class="reference internal image-reference" href="_images/buffer_lifecycle.png"><img alt="Diagram showing the buffer lifecycle" src="_images/buffer_lifecycle.png" style="height: 400px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4 </span><span class="caption-text">The buffer lifecycle</span><a class="headerlink" href="#id7" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Buffers are used to hold data internally within the server after receiving a request.
The implementations of buffers are in <code class="docutils literal notranslate"><span class="pre">src/proteus/buffers</span></code>.</p>
<p>The lifecycle of buffers is shown in <a class="reference internal" href="#fig-buffering"><span class="std std-numref">Fig. 4</span></a>.
In <code class="docutils literal notranslate"><span class="pre">allocate()</span></code>, the worker creates a buffer pool made of some number of buffers.
Using a buffer pool saves the cost of constantly allocating dynamic memory for each new request.
Instead, we can reuse the same set of buffers that are allocated by the worker at one time.
They are initially provided by the worker to the Manager which maintains a queue of buffers for storing the coalesced requests for one batch.
The buffers of all the workers in one group are maintained in this common queue.
They are consumed from the pool as the batcher creates batches and then the worker returns them to the pool after finishing work on a batch.
If the batcher needs a buffer but there are none available, the batcher can block execution until a buffer becomes available.
Thus, the number of buffers in the pool controls the number of active batches for a particular worker group.
Currently, there’s no mechanism to change the number of buffers in the pool at run-time short of allocating a new worker or sending a large request that forces the automatic allocation of more buffers.
In the future, the number of buffers may be controllable from the Manager and dynamically managed depending on the number of requests.</p>
<p>Multiple kinds of buffer backends are supported by providing the appropriate wrappers.
For example, a simple implementation may use buffers allocated in CPU memory.
For more advanced sharing of data and to minimize data movement, buffers may be allocated in shared memory or on hardware accelerators.
Buffer backends extend the <code class="docutils literal notranslate"><span class="pre">Buffer</span></code> class.
They provide methods to write different data types into the buffer and access the underlying data at some offset.</p>
</section>
<section id="manager">
<h2>Manager<a class="headerlink" href="#manager" title="Permalink to this heading">¶</a></h2>
<p>The shared state of Proteus is maintained by the Manager: the active workers, their buffer pools, the endpoints and load-time parameters associated with them and is visualized+ in <a class="reference internal" href="#architecture-detail"><span class="std std-numref">Fig. 2</span></a>.
This information enables the ingestion protocols to query the Manager to retrieve a pointer to the correct batcher to use to push the <code class="docutils literal notranslate"><span class="pre">Interface</span></code> object to the right one corresponding to the targeted worker.
To manage multiple versions of workers that may be running with different configurations, the Manager stores the load-time parameters, if any, and compares new parameters with ones its seen before to determine whether the newly loaded worker should be part of an existing worker group or a new one.
In the case that it’s assigned to an existing worker group, the previously allocated endpoint is returned to the client.
If a new worker group is created, a new endpoint is reserved for this worker group and returned to the client.
The implementation is in <code class="docutils literal notranslate"><span class="pre">src/proteus/core/manager.*</span></code>.</p>
<p>Loading a new worker results in the creation of a new <code class="docutils literal notranslate"><span class="pre">WorkerInfo</span></code> (see <code class="docutils literal notranslate"><span class="pre">src/proteus/core/worker_info.*</span></code>) object which the Manager uses internally to hold all the information associated with the worker.
The worker class instance, its batcher, and its buffer pool are all stored in this object.
The <code class="docutils literal notranslate"><span class="pre">WorkerInfo</span></code> object provides two methods to create new workers: its constructor and an <code class="docutils literal notranslate"><span class="pre">addAndStartWorker()</span></code> method.
The former is used for a brand-new worker and creates queues for the buffer pools and initializes the private members of the class.
The latter loads the shared library associated with the worker, creates and saves the instance of the worker class, and starts its <code class="docutils literal notranslate"><span class="pre">run()</span></code> method in a new thread.</p>
<p>The Manager also provides methods to safely modify the shared state such as loading or unloading new workers.
Such actions must be taken with care because there are many threads that may need to modify state or make decisions based on the current state.
The Manager uses a queue and a separate thread for this purpose.
All methods that modify state enqueue requests to this queue.
These methods may be called from a multi-threaded context and so multiple duplicate or contradictory requests are possible.
The queue enforces serialization and defines an ordering for all incoming requests so they can be processed in this order by the new thread.
Here, duplicate or contradictory requests can be silently dropped so the shared state isn’t corrupted.</p>
</section>
<section id="observation">
<h2>Observation<a class="headerlink" href="#observation" title="Permalink to this heading">¶</a></h2>
<p>Visibility into the server and its operations is provided through logging, metrics and tracing.
The implementations of these components is in <code class="docutils literal notranslate"><span class="pre">src/proteus/observation</span></code>.</p>
<section id="logging">
<h3>Logging<a class="headerlink" href="#logging" title="Permalink to this heading">¶</a></h3>
<p>The Server uses <a class="reference external" href="https://github.com/gabime/spdlog">spdlog</a> to provide logging.
By default, one logger is configured with <code class="docutils literal notranslate"><span class="pre">initLogging()</span></code>, which logs data to a file on the disk and prints warning messages to the terminal as well.
The preprocesser directive form of logging is used throughout the Server, which enables all logging data to be optionally removed at compile-time.</p>
<p>Look at <a class="reference internal" href="logging.html#logs"><span class="std std-ref">Logs</span></a> for more information.</p>
</section>
<section id="metrics">
<h3>Metrics<a class="headerlink" href="#metrics" title="Permalink to this heading">¶</a></h3>
<p>The Server uses <a class="reference external" href="https://github.com/jupp0r/prometheus-cpp">prometheus-cpp</a> to provide metric collection in the Prometheus format.
The metric data can be queried via the web server at the <code class="docutils literal notranslate"><span class="pre">/metrics</span></code> endpoint.
At compile-time, the metrics of interest must be defined in the <code class="docutils literal notranslate"><span class="pre">Metrics</span></code> class.
It provides methods for functions in other classes to modify the metric state.
Metric collection can be disabled at compile-time with a CMake option.</p>
<p>Look at <a class="reference internal" href="metrics.html#metrics"><span class="std std-ref">Metrics</span></a> for more information.</p>
</section>
<section id="tracing">
<h3>Tracing<a class="headerlink" href="#tracing" title="Permalink to this heading">¶</a></h3>
<p>The Server uses <a class="reference external" href="https://github.com/jaegertracing/jaeger-client-cpp">jaeger-client-cpp</a> <a class="footnote-reference brackets" href="#f2" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a> to provide tracing.
Tracing tracks the time taken for different sections of the architecture to process a single request.
This data can be visualized in the Jaeger UI.
Tracing data can be disabled at compile-time with a CMake option.</p>
<p>Look at <a class="reference internal" href="tracing.html#tracing"><span class="std std-ref">Tracing</span></a> for more information.</p>
<aside class="footnote brackets" id="f1" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>Some methods are only available through HTTP at this time. Using the C++ API requires compiling an application linked against <code class="docutils literal notranslate"><span class="pre">libproteus.so</span></code> rather than making requests to a server.</p>
</aside>
<aside class="footnote brackets" id="f2" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">2</a><span class="fn-bracket">]</span></span>
<p>This library is deprecating and will be replaced with OpenTelemetry as recommended by Jaeger.</p>
</aside>
<aside class="footnote brackets" id="f3" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">3</a><span class="fn-bracket">]</span></span>
<p>There are currently some restrictions on what may be run such as the number of input/output tensors.</p>
</aside>
</section>
</section>
</section>


           </div>
           
          </div>
          <footer>
<!-- Atalwar: Moved the footer code to layout.html to resolve conflict with the Xilinx template -->
</footer>

        </div>
      </div>


	  <!-- Sphinx Page Footer block -->
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="contributing.html" class="btn btn-neutral float-right" title="Contributing" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="python_api.html" class="btn btn-neutral float-left" title="Python API" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo" class="copyright">
    <p class="footerinfo">
      <span class="lastupdated">
        Last updated on July 28, 2022.
      </span>

    </p>
	<br>
  </div>
      </div>
    </section>


  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

   <script type="text/javascript">
    jQuery(function() { Search.loadIndex("searchindex.js"); });
  </script>

  <script type="text/javascript" id="searchindexloader"></script>


  
  
    
  



  <!--  Xilinx template footer block -->
							</div>
						</div>
					</div>
				</div>
				<div class="xilinxExperienceFragments experiencefragment aem-GridColumn--default--none aem-GridColumn aem-GridColumn--offset--default--0 aem-GridColumn--default--16">
					<div class="xf-content-height">
						<div class="aem-Grid aem-Grid--16 aem-Grid--default--16 ">
							<div class="footer parbase aem-GridColumn--default--none aem-GridColumn aem-GridColumn--offset--default--0 aem-GridColumn--default--16">
								<noindex>
                  <!-- make footer fixed - NileshP -->
                  <style>
                        .footer {
                        position: fixed;
                        left: 0;
                        bottom: 0;
                        width: 100%;
                        }
                  </style>
                  <!-- make footer fixed NileshP-->
									<footer>
										<div class="aem-Grid aem-Grid--16">
											<div class="aem-GridColumn aem-GridColumn--xxxlarge--none aem-GridColumn--xsmall--16 aem-GridColumn--offset--xsmall--0 aem-GridColumn--xlarge--none aem-GridColumn--xxlarge--none aem-GridColumn--default--none aem-GridColumn--offset--large--1 aem-GridColumn--xlarge--12 aem-GridColumn--offset--default--0 aem-GridColumn--xxlarge--10 aem-GridColumn--offset--xlarge--2 aem-GridColumn--offset--xxlarge--3 aem-GridColumn--offset--xxxlarge--4 aem-GridColumn--xsmall--none aem-GridColumn--large--none aem-GridColumn aem-GridColumn--large--14 aem-GridColumn--xxxlarge--8 aem-GridColumn--default--16">
												<div class="container-fluid sub-footer">
													<div class="row">
														<div class="footerSocial parbase">
														
															<div class="col-md-push-6 col-lg-push-6 col-md-6 col-lg-6">
																<div class="lang-select dropup hidden-md hidden-lg">
		<button data-toggle="dropdown">
			<span class="fas fa-globe" aria-hidden="true"></span>
			<span>
				
					English
				</span>
		<span class="far fa-angle-down" aria-hidden="true"></span>
	</button>
	<ul class="dropdown-menu">
		<li>
				<a target="_blank" href="https://japan.xilinx.com/" target="_self">
					日本語
				</a>
			</li>
		<li>
				<a target="_blank" href="https://china.xilinx.com/" target="_self">
					简体中文
				</a>
			</li>
		</ul>
	</div>
																<ul class="list-inline pull-right social-menu">
																	<li>
																		<a target="_blank" href="https://www.linkedin.com/company/xilinx">
																		<span class="linkedin icon"></span>
																		<span class="sr-only">Connect on LinkedIn</span>
																		</a>
																	</li>
																	<li>
																		<a target="_blank" href="https://www.twitter.com/XilinxInc">
																		<span class="twitter icon"></span>
																		<span class="sr-only">Follow us on Twitter</span>
																		</a>
																	</li>
																	<li>
																		<a target="_blank" href="https://www.facebook.com/XilinxInc">
																		<span class="facebook icon"></span>
																		<span class="sr-only">Connect on Facebook</span>
																		</a>
																	</li>
																	<li>
																		<a target="_blank" href="https://www.youtube.com/XilinxInc">
																		<span class="youtube icon"></span>
																		<span class="sr-only">Watch us on YouTube</span>
																		</a>
																	</li>
																	<li>
																		<a target="_blank" href="https://www.xilinx.com/registration/subscriber-signup.html">
																		<span class="newsletter icon"></span>
																		<span class="sr-only">Subscribe to Newsletter</span>
																		</a>
																	</li>
																</ul>
																	<div class="lang-select dropup hidden-xs hidden-sm">
	<button data-toggle="dropdown">
		<span class="fas fa-globe" aria-hidden="true"></span>
		<span>
			
				English
			</span>
		<span class="far fa-angle-down" aria-hidden="true"></span>
	</button>
	<ul class="dropdown-menu">
		<li>
				<a target="_blank" href="https://japan.xilinx.com/" target="_self">
					日本語
				</a>
			</li>
		<li>
				<a target="_blank" href="https://china.xilinx.com/" target="_self">
					简体中文
				</a>
			</li>
		</ul>
	</div>
															</div>
														</div>
														<div class="col-md-pull-5 col-lg-pull-5 col-md-5 col-lg-5">
															<span class="copyright">©2022 Advanced Micro Devices, Inc</span>
														</div>

													</div>
													                    <div class="movethisrowtoleft row">
                        <div class="col-xs-24">
                            <ul class="sub-menu">
                                <li><a target="_blank" href="https://www.amd.com/en/corporate/copyright">Terms and Conditions</a></li>
                                <li><a target="_blank" href="https://www.amd.com/en/corporate/privacy">Privacy</a></li>
                                <li><a target="_blank" href="https://www.amd.com/en/corporate/cookies">Cookie Policy</a></li>
                                <li><a target="_blank" href="https://www.amd.com/en/corporate/trademarks">Trademarks</a></li>
                                <li><a target="_blank" href="https://www.amd.com/system/files/documents/statement-human-trafficking-forced-labor.pdf">Statement on Forced Labor</a></li>
                                <li><a target="_blank" href="https://www.amd.com/en/corporate/competition">Fair and Open Competition</a></li>
                                <li><a target="_blank" href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf">UK Tax Strategy</a></li>
								<li><a target="_blank" href="https://docs.xilinx.com/v/u/9x6YvZKuWyhJId7y7RQQKA">Inclusive Terminology</a></li>
                                <li><a href="#cookiessettings" class="ot-sdk-show-settings">Cookies Settings</a></li>
                            </ul>
                        </div>
                    </div>
												</div>
											</div>
										</div>
									</footer>
								</noindex>
							</div>
						</div>
					</div>
				</div>
<div class="backToTop parbase aem-GridColumn--default--none aem-GridColumn aem-GridColumn--offset--default--0 aem-GridColumn--default--16"><noindex>
    <span data-component="backToTopButton" class="backToTopButton loaded">
        <ul>
            <li>
                <a href="https://www.author.xilinx.com/xx/rebrand/amd/en-amd-xilinx-header-footer.html#top" class="btn top">
                    <span class="fas fa-angle-up" aria-hidden="true"></span>
                </a>
            </li>
        </ul>
    </span>
</noindex></div>
			</div>
		</div>


		<script>window.CQ = window.CQ || {}</script>
		<script src="https://static.cloud.coveo.com/searchui/v2.4382/js/CoveoJsSearch.Lazy.min.js"></script>
		<script>
			var underscoreSetup = function () {
			  _.templateSettings.interpolate = /\{\{=([^-][\S\s]+?)\}\}/g;
			  _.templateSettings.evaluate = /\{\{([^-=][\S\s]+?)\}\}/g;
			  _.templateSettings.escape = /\{\{-([^=][\S\s]+?)\}\}/g;
			}

			underscoreSetup();
		</script>
	</body>
</html>