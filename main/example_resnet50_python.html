<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
<!-- OneTrust Cookies Consent Notice start for xilinx.github.io -->

<script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-document-language="true" type="text/javascript" charset="UTF-8" data-domain-script="03af8d57-0a04-47a6-8f10-322fa00d8fc7" ></script>
<script type="text/javascript">
function OptanonWrapper() { }
</script>
<!-- OneTrust Cookies Consent Notice end for xilinx.github.io -->
<!-- Google Tag Manager -->
<script type="text/plain" class="optanon-category-C0002">(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-5RHQV7');</script>
<!-- End Google Tag Manager -->
  <title>Running ResNet50 - Python &mdash; AMD Inference Server v0.3.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/tabs.css" type="text/css" />
      <link rel="stylesheet" href="_static/bootstrap-treeview/bootstrap-treeview.min.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="_static/bootstrap-treeview/bootstrap-treeview.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Platforms" href="platforms.html" />
    <link rel="prev" title="Running ResNet50 - C++" href="example_resnet50_cpp.html" /> 
</head>

<body class="wy-body-for-nav">

<!-- Google Tag Manager -->
<noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-5RHQV7" height="0" width="0" style="display:none;visibility:hidden" class="optanon-category-C0002"></iframe></noscript>
<!-- End Google Tag Manager --> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
            <a href="index.html" class="icon icon-home"> AMD Inference Server
          </a>
              <div class="version">
                main
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">About</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="introduction.html#features">Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="introduction.html#documentation-overview">Documentation overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="introduction.html#support">Support</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="dependencies.html">Dependencies</a><ul>
<li class="toctree-l2"><a class="reference internal" href="dependencies.html#docker-image">Docker Image</a><ul>
<li class="toctree-l3"><a class="reference internal" href="dependencies.html#base-image">Base Image</a></li>
<li class="toctree-l3"><a class="reference internal" href="dependencies.html#ubuntu-focal-repositories">Ubuntu Focal Repositories</a></li>
<li class="toctree-l3"><a class="reference internal" href="dependencies.html#ubuntu-ppas">Ubuntu PPAs</a></li>
<li class="toctree-l3"><a class="reference internal" href="dependencies.html#pypi">PyPI</a></li>
<li class="toctree-l3"><a class="reference internal" href="dependencies.html#github">Github</a></li>
<li class="toctree-l3"><a class="reference internal" href="dependencies.html#others">Others</a></li>
<li class="toctree-l3"><a class="reference internal" href="dependencies.html#xilinx">Xilinx</a></li>
<li class="toctree-l3"><a class="reference internal" href="dependencies.html#amd">AMD</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="dependencies.html#included">Included</a></li>
<li class="toctree-l2"><a class="reference internal" href="dependencies.html#downloaded-files">Downloaded Files</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="roadmap.html">Roadmap</a><ul>
<li class="toctree-l2"><a class="reference internal" href="roadmap.html#q1">2022 Q1</a></li>
<li class="toctree-l2"><a class="reference internal" href="roadmap.html#q2">2022 Q2</a></li>
<li class="toctree-l2"><a class="reference internal" href="roadmap.html#q3">2022 Q3</a></li>
<li class="toctree-l2"><a class="reference internal" href="roadmap.html#future">Future</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="changelog.html">Changelog</a><ul>
<li class="toctree-l2"><a class="reference internal" href="changelog.html#unreleased">Unreleased</a><ul>
<li class="toctree-l3"><a class="reference internal" href="changelog.html#added">Added</a></li>
<li class="toctree-l3"><a class="reference internal" href="changelog.html#changed">Changed</a></li>
<li class="toctree-l3"><a class="reference internal" href="changelog.html#fixed">Fixed</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="changelog.html#id2">0.2.0 - 2022-08-05</a><ul>
<li class="toctree-l3"><a class="reference internal" href="changelog.html#id3">Added</a></li>
<li class="toctree-l3"><a class="reference internal" href="changelog.html#id4">Changed</a></li>
<li class="toctree-l3"><a class="reference internal" href="changelog.html#id5">Fixed</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="changelog.html#id6">0.1.0 - 2022-02-08</a><ul>
<li class="toctree-l3"><a class="reference internal" href="changelog.html#id7">Added</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quickstart</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quickstart_inference.html">Inference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="quickstart_inference.html#making-requests-with-the-library">Making requests with the library</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart_inference.html#making-requests-directly">Making requests directly</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="quickstart_deployment.html">Deployment</a><ul>
<li class="toctree-l2"><a class="reference internal" href="quickstart_deployment.html#deployment-vs-development-images">Deployment vs. Development Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart_deployment.html#deploying-on-docker">Deploying on Docker</a><ul>
<li class="toctree-l3"><a class="reference internal" href="quickstart_deployment.html#deployment-image">Deployment image</a></li>
<li class="toctree-l3"><a class="reference internal" href="quickstart_deployment.html#development-image">Development image</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="quickstart_deployment.html#deploying-on-kserve">Deploying on KServe</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart_deployment.html#deploying-without-docker">Deploying without Docker</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart_deployment.html#loading-models">Loading models</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="quickstart_development.html">Development</a><ul>
<li class="toctree-l2"><a class="reference internal" href="quickstart_development.html#get-the-code">Get the code</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart_development.html#build-or-get-the-docker-image">Build or get the Docker image</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart_development.html#compiling-the-amd-inference-server">Compiling the AMD Inference Server</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart_development.html#get-test-artifacts">Get test artifacts</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart_development.html#run-the-amd-inference-server">Run the AMD Inference Server</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries and API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cpp_user_api.html">C++</a><ul>
<li class="toctree-l2"><a class="reference internal" href="cpp_user_api.html#clients">Clients</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cpp_user_api.html#grpc">gRPC</a></li>
<li class="toctree-l3"><a class="reference internal" href="cpp_user_api.html#http">HTTP</a></li>
<li class="toctree-l3"><a class="reference internal" href="cpp_user_api.html#native">Native</a></li>
<li class="toctree-l3"><a class="reference internal" href="cpp_user_api.html#websocket">WebSocket</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="cpp_user_api.html#core">Core</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cpp_user_api.html#datatype">DataType</a></li>
<li class="toctree-l3"><a class="reference internal" href="cpp_user_api.html#exceptions">Exceptions</a></li>
<li class="toctree-l3"><a class="reference internal" href="cpp_user_api.html#prediction">Prediction</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="cpp_user_api.html#servers">Servers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="python.html">Python</a><ul>
<li class="toctree-l2"><a class="reference internal" href="python.html#install-the-python-library">Install the Python library</a><ul>
<li class="toctree-l3"><a class="reference internal" href="python.html#build-wheels">Build wheels</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="python.html#module-amdinfer">API</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="rest.html">REST Endpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">Command-Line Interface</a><ul>
<li class="toctree-l2"><a class="reference internal" href="cli.html#commands">Commands</a></li>
<li class="toctree-l2"><a class="reference internal" href="cli.html#options">Options</a></li>
<li class="toctree-l2"><a class="reference internal" href="cli.html#Sub-commands">Sub-commands</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cli.html#attach">attach</a></li>
<li class="toctree-l3"><a class="reference internal" href="cli.html#benchmark">benchmark</a></li>
<li class="toctree-l3"><a class="reference internal" href="cli.html#build">build</a></li>
<li class="toctree-l3"><a class="reference internal" href="cli.html#clean">clean</a></li>
<li class="toctree-l3"><a class="reference internal" href="cli.html#dockerize">dockerize</a></li>
<li class="toctree-l3"><a class="reference internal" href="cli.html#get">get</a></li>
<li class="toctree-l3"><a class="reference internal" href="cli.html#install">install</a></li>
<li class="toctree-l3"><a class="reference internal" href="cli.html#list">list</a></li>
<li class="toctree-l3"><a class="reference internal" href="cli.html#make">make</a></li>
<li class="toctree-l3"><a class="reference internal" href="cli.html#run">run</a></li>
<li class="toctree-l3"><a class="reference internal" href="cli.html#start">start</a></li>
<li class="toctree-l3"><a class="reference internal" href="cli.html#test">test</a></li>
<li class="toctree-l3"><a class="reference internal" href="cli.html#up">up</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="hello_world_echo.html">Hello World - Python</a><ul>
<li class="toctree-l2"><a class="reference internal" href="hello_world_echo.html#import-the-library">Import the library</a></li>
<li class="toctree-l2"><a class="reference internal" href="hello_world_echo.html#create-our-client-and-server-objects">Create our client and server objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="hello_world_echo.html#is-amd-inference-server-already-running">Is AMD Inference Server already running?</a></li>
<li class="toctree-l2"><a class="reference internal" href="hello_world_echo.html#load-a-worker">Load a worker</a></li>
<li class="toctree-l2"><a class="reference internal" href="hello_world_echo.html#inference">Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="hello_world_echo.html#validate-the-response">Validate the response</a></li>
<li class="toctree-l2"><a class="reference internal" href="hello_world_echo.html#clean-up">Clean up</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="example_resnet50_cpp.html">Running ResNet50 - C++</a><ul>
<li class="toctree-l2"><a class="reference internal" href="example_resnet50_cpp.html#include-the-header">Include the header</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_resnet50_cpp.html#start-the-server">Start the server</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_resnet50_cpp.html#create-the-client-object">Create the client object</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_resnet50_cpp.html#load-a-worker">Load a worker</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_resnet50_cpp.html#prepare-images">Prepare images</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_resnet50_cpp.html#construct-requests">Construct requests</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_resnet50_cpp.html#make-an-inference">Make an inference</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Running ResNet50 - Python</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#include-the-module">Include the module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#start-the-server">Start the server</a></li>
<li class="toctree-l2"><a class="reference internal" href="#create-the-client-object">Create the client object</a></li>
<li class="toctree-l2"><a class="reference internal" href="#load-a-worker">Load a worker</a></li>
<li class="toctree-l2"><a class="reference internal" href="#prepare-images">Prepare images</a></li>
<li class="toctree-l2"><a class="reference internal" href="#construct-requests">Construct requests</a></li>
<li class="toctree-l2"><a class="reference internal" href="#make-an-inference">Make an inference</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Using the Server</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="platforms.html">Platforms</a><ul>
<li class="toctree-l2"><a class="reference internal" href="zendnn.html">CPUs - ZenDNN</a><ul>
<li class="toctree-l3"><a class="reference internal" href="zendnn.html#build-an-image">Build an image</a></li>
<li class="toctree-l3"><a class="reference internal" href="zendnn.html#get-assets-and-models">Get assets and models</a></li>
<li class="toctree-l3"><a class="reference internal" href="zendnn.html#freezing-pytorch-models">Freezing PyTorch models</a></li>
<li class="toctree-l3"><a class="reference internal" href="zendnn.html#run-tests">Run Tests</a></li>
<li class="toctree-l3"><a class="reference internal" href="zendnn.html#tune-performance">Tune performance</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="migraphx.html">GPUs - MIGraphX</a><ul>
<li class="toctree-l3"><a class="reference internal" href="migraphx.html#set-up-the-host-and-gpus">Set up the host and GPUs</a></li>
<li class="toctree-l3"><a class="reference internal" href="migraphx.html#build-an-image">Build an image</a></li>
<li class="toctree-l3"><a class="reference internal" href="migraphx.html#start-an-image">Start an image</a></li>
<li class="toctree-l3"><a class="reference internal" href="migraphx.html#get-assets-and-models">Get assets and models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="vitis_ai.html">FPGAs - Vitis AI</a><ul>
<li class="toctree-l3"><a class="reference internal" href="vitis_ai.html#set-up-the-host-and-fpgas">Set up the host and FPGAs</a></li>
<li class="toctree-l3"><a class="reference internal" href="vitis_ai.html#build-an-image">Build an image</a></li>
<li class="toctree-l3"><a class="reference internal" href="vitis_ai.html#start-an-image">Start an image</a></li>
<li class="toctree-l3"><a class="reference internal" href="vitis_ai.html#get-assets-and-models">Get assets and models</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="docker.html">Deploying with Docker</a><ul>
<li class="toctree-l2"><a class="reference internal" href="docker.html#build-the-production-docker-image">Build the production Docker image</a><ul>
<li class="toctree-l3"><a class="reference internal" href="docker.html#push-to-a-registry">Push to a registry</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="docker.html#prepare-the-image-for-docker-deployment">Prepare the image for Docker deployment</a></li>
<li class="toctree-l2"><a class="reference internal" href="docker.html#start-the-container">Start the container</a></li>
<li class="toctree-l2"><a class="reference internal" href="docker.html#make-a-request">Make a request</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="kserve.html">Deploying with KServe</a><ul>
<li class="toctree-l2"><a class="reference internal" href="kserve.html#set-up-kubernetes-and-kserve">Set up Kubernetes and KServe</a></li>
<li class="toctree-l2"><a class="reference internal" href="kserve.html#get-or-build-the-amd-inference-server-image">Get or build the AMD Inference Server Image</a></li>
<li class="toctree-l2"><a class="reference internal" href="kserve.html#start-an-inference-service">Start an inference service</a><ul>
<li class="toctree-l3"><a class="reference internal" href="kserve.html#serving-runtime">Serving Runtime</a></li>
<li class="toctree-l3"><a class="reference internal" href="kserve.html#custom-container">Custom container</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="kserve.html#making-requests">Making Requests</a></li>
<li class="toctree-l2"><a class="reference internal" href="kserve.html#debugging">Debugging</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="performance_factors.html">Performance Factors</a><ul>
<li class="toctree-l2"><a class="reference internal" href="performance_factors.html#hardware">Hardware</a></li>
<li class="toctree-l2"><a class="reference internal" href="performance_factors.html#compile-the-right-version">Compile the right version</a></li>
<li class="toctree-l2"><a class="reference internal" href="performance_factors.html#parallelism">Parallelism</a><ul>
<li class="toctree-l3"><a class="reference internal" href="performance_factors.html#rest-threads">REST threads</a></li>
<li class="toctree-l3"><a class="reference internal" href="performance_factors.html#sending-requests">Sending requests</a></li>
<li class="toctree-l3"><a class="reference internal" href="performance_factors.html#duplicating-workers">Duplicating workers</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#ways-to-contribute">Ways to Contribute</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#contributing-code">Contributing Code</a><ul>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#sign-your-work">Sign Your Work</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#style-guide">Style Guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#documentation">Documentation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Architecture</a><ul>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#ingestion">Ingestion</a><ul>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#api">API</a></li>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#http-rest-and-websocket">HTTP/REST and WebSocket</a></li>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#c-api">C++ API</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#batching">Batching</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#workers">Workers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#organization-and-lifecycle">Organization and Lifecycle</a></li>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#improving-performance">Improving Performance</a></li>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#external-processing">External Processing</a></li>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#xmodel">XModel</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#buffering">Buffering</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#manager">Manager</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#observation">Observation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#logging">Logging</a></li>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#metrics">Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#tracing">Tracing</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="aks.html">AKS</a><ul>
<li class="toctree-l2"><a class="reference internal" href="aks.html#introduction-to-aks">Introduction to AKS</a></li>
<li class="toctree-l2"><a class="reference internal" href="aks.html#using-aks-in-amd-inference-server">Using AKS in AMD Inference Server</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">Logs</a><ul>
<li class="toctree-l2"><a class="reference internal" href="logging.html#amd-inference-server-logs">AMD Inference Server Logs</a></li>
<li class="toctree-l2"><a class="reference internal" href="logging.html#drogon-logs">Drogon Logs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="benchmarking.html">Benchmarking</a><ul>
<li class="toctree-l2"><a class="reference internal" href="benchmarking.html#xmodel-benchmarking">XModel Benchmarking</a></li>
<li class="toctree-l2"><a class="reference internal" href="benchmarking.html#kernel-simulation">Kernel Simulation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="metrics.html">Metrics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="metrics.html#quickstart">Quickstart</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tracing.html">Tracing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="tracing.html#quickstart">Quickstart</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="cpp_api/cpp_root.html">Code Documentation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="cpp_api/cpp_root.html#full-api">Full API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_root.html#namespaces">Namespaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_root.html#classes-and-structs">Classes and Structs</a></li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_root.html#enums">Enums</a></li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_root.html#functions">Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_root.html#files">Files</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">AMD Inference Server</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Running ResNet50 - Python</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/example_resnet50_python.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="running-resnet50-python">
<h1>Running ResNet50 - Python<a class="headerlink" href="#running-resnet50-python" title="Permalink to this headline">¶</a></h1>
<p>This page walks you through the Python versions of the ResNet50 examples.
These examples and script are intended to run in the dev container.
You can see the full source files in the <a class="reference external" href="https://github.com/Xilinx/inference-server/tree/main/examples/resnet50">repository</a> for more details.</p>
<p>The inference server binds its C++ API to Python so the Python usage and functions look similar to their C++ counterparts but there are some differences due to the available features in both languages.
You should read the C++ version of this documentation to better compare these two.</p>
<p>In the dev container, the Python library is automatically built and installed as part of the CMake build process.
You may also install the Python library by installing a wheel.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>These examples are intended to demonstrate the API and how to communicate with the server.
They are not intended to show the most optimal performance for each backend.</p>
</div>
<section id="include-the-module">
<h2>Include the module<a class="headerlink" href="#include-the-module" title="Permalink to this headline">¶</a></h2>
<p>You can include the Python library by importing the <code class="docutils literal notranslate"><span class="pre">amdinfer</span></code> module.
The different submodules are imported by default but you can include them manually as well.
These explicit imports can help IDEs resolve Python imports for autocompletion.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">amdinfer</span>
<span class="kn">import</span> <span class="nn">amdinfer.pre_post</span> <span class="k">as</span> <span class="nn">pre_post</span>

</pre></div>
</div>
</section>
<section id="start-the-server">
<h2>Start the server<a class="headerlink" href="#start-the-server" title="Permalink to this headline">¶</a></h2>
<p>This example assumes that the server is not running elsewhere and starts it locally from Python instead.
Creating the <code class="docutils literal notranslate"><span class="pre">amdinfer.Server</span></code> object starts the inference server backend and it stays alive as long as the object remains in scope.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">server</span> <span class="o">=</span> <span class="n">amdinfer</span><span class="o">.</span><span class="n">Server</span><span class="p">()</span>
</pre></div>
</div>
<p>Depending on what protocol you want to use to communicate with the server, you may need to start it explicitly using one of the object’s methods.</p>
<p>For example, if you were using HTTP/REST:</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">server</span><span class="o">.</span><span class="n">startHttp</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">http_port</span><span class="p">)</span>
</pre></div>
</div>
<p>If the server is already running somewhere, you don’t need to do this.</p>
</section>
<section id="create-the-client-object">
<h2>Create the client object<a class="headerlink" href="#create-the-client-object" title="Permalink to this headline">¶</a></h2>
<p>As in C++, you can create a client in Python corresponding to the protocol you want to use to talk to the server.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">server_addr</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;http://</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">ip</span><span class="si">}</span><span class="s2">:</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">http_port</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">amdinfer</span><span class="o">.</span><span class="n">HttpClient</span><span class="p">(</span><span class="n">server_addr</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="load-a-worker">
<h2>Load a worker<a class="headerlink" href="#load-a-worker" title="Permalink to this headline">¶</a></h2>
<p>Once the server is ready, you have to load a worker to handle your request.
This worker, and any load-time parameters it accepts, are backend-specific.
After loading the worker, you get back an endpoint string that you use to make requests to this worker.
If a worker is already ready on the server or you already have an endpoint, then you don’t need to do this.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">amdinfer</span><span class="o">.</span><span class="n">RequestParameters</span><span class="p">()</span>
<span class="n">parameters</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="s2">&quot;model&quot;</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
<span class="n">parameters</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="s2">&quot;batch_size&quot;</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">endpoint</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">workerLoad</span><span class="p">(</span><span class="s2">&quot;xmodel&quot;</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
</pre></div>
</div>
<p>After loading a worker, make sure it’s ready before attempting to make an inference.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">amdinfer</span><span class="o">.</span><span class="n">waitUntilModelReady</span><span class="p">(</span><span class="n">client</span><span class="p">,</span> <span class="n">endpoint</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="prepare-images">
<h2>Prepare images<a class="headerlink" href="#prepare-images" title="Permalink to this headline">¶</a></h2>
<p>Depending on the model, you may need to perform some preprocessing of the data before making an inference request.
For ResNet50, this preprocessing generally consists of resizing the image, normalizing its values, and possibly converting types but its exact implementation depends on the model and on what the worker expects.
The implementations of the preprocessing functions can be seen in the examples’ sources and they differ for each backend.</p>
<p>In these examples, you can pass a path to an image or to a directory to the executable.
This single path gets converted to a List of paths containing just the path you passed in or paths to all the files in the directory you passed in and its passed to the <code class="docutils literal notranslate"><span class="pre">preprocess</span></code> function.
The file at each path is opened as a <code class="docutils literal notranslate"><span class="pre">numpy</span></code> array and stored in a List that the preprocess function returns.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">paths</span> <span class="o">=</span> <span class="n">resolve_image_paths</span><span class="p">(</span><span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">image</span><span class="p">))</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">paths</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="construct-requests">
<h2>Construct requests<a class="headerlink" href="#construct-requests" title="Permalink to this headline">¶</a></h2>
<p>Using the images after preprocessing, you can construct requests to the inference server.
Each image is constructed into an <code class="docutils literal notranslate"><span class="pre">InferenceRequest</span></code> using the <code class="docutils literal notranslate"><span class="pre">ImageInferenceRequest</span></code> helper function.
This function accepts a single <code class="docutils literal notranslate"><span class="pre">numpy</span></code> array or a list of <code class="docutils literal notranslate"><span class="pre">numpy</span></code> arrays, where each array represents an input tensor.
The ResNet50 model only accepts a single input tensor so a single image is enough.
This function infers the shape and datatype of the image using the properties stored in the <code class="docutils literal notranslate"><span class="pre">numpy</span></code> array.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">construct_requests</span><span class="p">(</span><span class="n">images</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Construct requests for the inference server from the input images. For ResNet50,</span>
<span class="sd">    a valid request includes a single input tensor containing a square image.</span>

<span class="sd">    Args:</span>
<span class="sd">        images (list[numpy.ndarray]): the input images</span>

<span class="sd">    Returns:</span>
<span class="sd">        list[amdinfer.InferenceRequest]: the requests</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">requests</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">images</span><span class="p">:</span>
        <span class="n">requests</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">amdinfer</span><span class="o">.</span><span class="n">ImageInferenceRequest</span><span class="p">(</span><span class="n">image</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">requests</span>


</pre></div>
</div>
</section>
<section id="make-an-inference">
<h2>Make an inference<a class="headerlink" href="#make-an-inference" title="Permalink to this headline">¶</a></h2>
<p>There are multiple ways of making a request to the inference server, some of which are used in the different implementations of these examples.
Before processing the response, you should verify it’s not an error.</p>
<p>Then, you can examine the outputs and, depending on the model, postprocess the results.
For ResNet50, the raw results from the inference server lists the probabilities for each output class.
The postprocessing identifies the highest probability classes and the top few of these are printed using the labels file to map the indices to a human-readable name.</p>
<p>Here are some examples of making an inference used in these examples:</p>
<p>This is the simplest way to make an inference: a blocking single inference where you loop through all the requests and make requests to the server one at a time.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="c1"># in vitis.py</span>
<span class="k">for</span> <span class="n">image_path</span><span class="p">,</span> <span class="n">request</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">paths</span><span class="p">,</span> <span class="n">requests</span><span class="p">):</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">modelInfer</span><span class="p">(</span><span class="n">endpoint</span><span class="p">,</span> <span class="n">request</span><span class="p">)</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">response</span><span class="o">.</span><span class="n">isError</span><span class="p">()</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">getOutputs</span><span class="p">()</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="n">top_indices</span> <span class="o">=</span> <span class="n">postprocess</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">args</span><span class="o">.</span><span class="n">top</span><span class="p">)</span>
    <span class="n">print_label</span><span class="p">(</span><span class="n">top_indices</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">labels</span><span class="p">,</span> <span class="n">image_path</span><span class="p">)</span>
</pre></div>
</div>
<p>There are also some helper methods that wrap the basic inference APIs provided by the client.
The <code class="docutils literal notranslate"><span class="pre">inferAsyncOrdered</span></code> method accepts a list of requests, makes all the requests asynchronously using the C++ <code class="docutils literal notranslate"><span class="pre">modelInferAsync</span></code> API, waits until each request completes, and then returns a list of responses.
If there are multiple requests sent in this way, they may be batched together by the server.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="c1"># in migraphx.py</span>
<span class="n">responses</span> <span class="o">=</span> <span class="n">amdinfer</span><span class="o">.</span><span class="n">inferAsyncOrdered</span><span class="p">(</span><span class="n">client</span><span class="p">,</span> <span class="n">endpoint</span><span class="p">,</span> <span class="n">requests</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Making inferences...&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">image_path</span><span class="p">,</span> <span class="n">response</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">paths</span><span class="p">,</span> <span class="n">responses</span><span class="p">):</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">response</span><span class="o">.</span><span class="n">isError</span><span class="p">()</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">getOutputs</span><span class="p">()</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="n">top_indices</span> <span class="o">=</span> <span class="n">postprocess</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">args</span><span class="o">.</span><span class="n">top</span><span class="p">)</span>
    <span class="n">print_label</span><span class="p">(</span><span class="n">top_indices</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">labels</span><span class="p">,</span> <span class="n">image_path</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          
                  <style>
                        .footer {
                        position: fixed;
                        left: 0;
                        bottom: 0;
                        width: 100%;
                        }
                  </style>
				  
				  <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="example_resnet50_cpp.html" class="btn btn-neutral float-left" title="Running ResNet50 - C++" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="platforms.html" class="btn btn-neutral float-right" title="Platforms" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022 Advanced Micro Devices, Inc..
      <span class="lastupdated">Last updated on December 15, 2022.
      </span></p>
  </div>



										<div class="aem-Grid aem-Grid--16">
											<div class="aem-GridColumn aem-GridColumn--xxxlarge--none aem-GridColumn--xsmall--16 aem-GridColumn--offset--xsmall--0 aem-GridColumn--xlarge--none aem-GridColumn--xxlarge--none aem-GridColumn--default--none aem-GridColumn--offset--large--1 aem-GridColumn--xlarge--12 aem-GridColumn--offset--default--0 aem-GridColumn--xxlarge--10 aem-GridColumn--offset--xlarge--2 aem-GridColumn--offset--xxlarge--3 aem-GridColumn--offset--xxxlarge--4 aem-GridColumn--xsmall--none aem-GridColumn--large--none aem-GridColumn aem-GridColumn--large--14 aem-GridColumn--xxxlarge--8 aem-GridColumn--default--16">
												<div class="container-fluid sub-footer">

													                    <div class="row">
                        <div class="col-xs-24">
                          <p><a target="_blank" href="https://www.amd.com/en/corporate/copyright">Terms and Conditions</a> | <a target="_blank" href="https://www.amd.com/en/corporate/privacy">Privacy</a> | <a target="_blank" href="https://www.amd.com/en/corporate/cookies">Cookie Policy</a> | <a target="_blank" href="https://www.amd.com/en/corporate/trademarks">Trademarks</a> | <a target="_blank" href="https://www.amd.com/system/files/documents/statement-human-trafficking-forced-labor.pdf">Statement on Forced Labor</a> | <a target="_blank" href="https://www.amd.com/en/corporate/competition">Fair and Open Competition</a> | <a target="_blank" href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf">UK Tax Strategy</a> | <a target="_blank" href="https://docs.xilinx.com/v/u/9x6YvZKuWyhJId7y7RQQKA">Inclusive Terminology</a> | <a href="#cookiessettings" class="ot-sdk-show-settings">Cookies Settings</a></p>
                        </div>
                    </div>
												</div>
											</div>
										</div>
										
</br>


  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  

  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      <span class="fa fa-book"> Read the Docs</span>
      v: main
      <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
      
      <dl>
        <dt>Languages</dt>
        
           <strong> 
          <dd><a href="/inference-server/main/">en</a></dd>
           </strong> 
        
      </dl>
      
      
      <dl>
        <dt>Versions</dt>
        
          
          <dd><a href="/inference-server/0.1.0/">0.1.0</a></dd>
          
        
          
          <dd><a href="/inference-server/0.2.0/">0.2.0</a></dd>
          
        
           <strong> 
          <dd><a href="/inference-server/main/">main</a></dd>
           </strong> 
        
      </dl>
      
      
       
    </div>
  </div>
<script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(false);
      });
  </script>
 <script type="text/javascript">
    $(document).ready(function() {
        $(".toggle > *").hide();
        $(".toggle .header").show();
        $(".toggle .header").click(function() {
            $(this).parent().children().not(".header").toggle(400);
            $(this).parent().children(".header").toggleClass("open");
        })
    });
</script>


</body>
</html>