<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
<!-- OneTrust Cookies Consent Notice start for xilinx.github.io -->

<script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-document-language="true" type="text/javascript" charset="UTF-8" data-domain-script="03af8d57-0a04-47a6-8f10-322fa00d8fc7" ></script>
<script type="text/javascript">
function OptanonWrapper() { }
</script>
<!-- OneTrust Cookies Consent Notice end for xilinx.github.io -->
<!-- Google Tag Manager -->
<script type="text/plain" class="optanon-category-C0002">(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-5RHQV7');</script>
<!-- End Google Tag Manager -->
  <title>Running ResNet50 - C++ &mdash; AMD Inference Server v0.3.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/bootstrap-treeview/bootstrap-treeview.min.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="_static/bootstrap-treeview/bootstrap-treeview.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Running ResNet50 - Python" href="example_resnet50_python.html" />
    <link rel="prev" title="Hello World - Python" href="hello_world_echo.html" /> 
</head>

<body class="wy-body-for-nav">

<!-- Google Tag Manager -->
<noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-5RHQV7" height="0" width="0" style="display:none;visibility:hidden" class="optanon-category-C0002"></iframe></noscript>
<!-- End Google Tag Manager --> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
            <a href="index.html" class="icon icon-home"> AMD Inference Server
          </a>
              <div class="version">
                main
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="index.html">AMD Inference Server</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="hello_world_echo.html">Hello World - Python</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Running ResNet50 - C++</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#include-the-header">Include the header</a></li>
<li class="toctree-l2"><a class="reference internal" href="#start-the-server">Start the server</a></li>
<li class="toctree-l2"><a class="reference internal" href="#create-the-client-object">Create the client object</a></li>
<li class="toctree-l2"><a class="reference internal" href="#load-a-worker">Load a worker</a></li>
<li class="toctree-l2"><a class="reference internal" href="#prepare-images">Prepare images</a></li>
<li class="toctree-l2"><a class="reference internal" href="#construct-requests">Construct requests</a></li>
<li class="toctree-l2"><a class="reference internal" href="#make-an-inference">Make an inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="example_resnet50_python.html">Running ResNet50 - Python</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Using AMD Inference Server</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="aks.html">AKS</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">Logs</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmarking.html">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="metrics.html">Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="tracing.html">Tracing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Platforms</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="zendnn.html">CPUs - ZenDNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="migraphx.html">GPUs - MIGraphX</a></li>
<li class="toctree-l1"><a class="reference internal" href="vitis_ai.html">FPGAs - Vitis AI</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Deployment</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="docker.html">Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="kserve.html">KServe</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Getting Performance</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="performance_factors.html">Factors</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User API libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="rest.html">REST Endpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_user_api.html">C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="python.html">Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">Command-Line Interface</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_api/cpp_root.html">Code Documentation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">About</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="roadmap.html">Roadmap</a></li>
<li class="toctree-l1"><a class="reference internal" href="changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="dependencies.html">Dependencies</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">AMD Inference Server</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Running ResNet50 - C++</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/example_resnet50_cpp.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="running-resnet50-c">
<h1>Running ResNet50 - C++<a class="headerlink" href="#running-resnet50-c" title="Permalink to this headline">¶</a></h1>
<p>This page walks you through the C++ versions of the ResNet50 examples.
These examples are intended to run in the dev container because you need to build and compile these executables.
You can see the full source files used here in the repository for more details.</p>
<p>While the inference server supports many backends, client applications to make requests to any of them largely look the same.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>These examples are intended to demonstrate the API and how to communicate with the server.
They are not intended to show the most optimal performance for each backend.</p>
</div>
<section id="include-the-header">
<h2>Include the header<a class="headerlink" href="#include-the-header" title="Permalink to this headline">¶</a></h2>
<p>AMD Inference Server’s C++ API allows you to write your own C++ client applications that can communicate with the inference server.
You can include the entire public API by including <code class="file docutils literal notranslate"><span class="pre">amdinfer/amdinfer.hpp</span></code> or selectively include header files as needed.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;amdinfer/amdinfer.hpp&quot;</span><span class="cp"></span>
</pre></div>
</div>
</section>
<section id="start-the-server">
<h2>Start the server<a class="headerlink" href="#start-the-server" title="Permalink to this headline">¶</a></h2>
<p>This example assumes that the server is not running elsewhere and starts it locally from C++ instead.
Creating the <code class="docutils literal notranslate"><span class="pre">amdinfer::Server</span></code> object starts the inference server backend and it stays alive as long as the object remains in scope.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">amdinfer</span><span class="o">::</span><span class="n">Server</span><span class="o">&gt;</span><span class="w"> </span><span class="n">server</span><span class="p">;</span><span class="w"></span>
</pre></div>
</div>
<p>Depending on what protocol you want to use to communicate with the server, you may need to start it explicitly using one of the <code class="docutils literal notranslate"><span class="pre">amdinfer::Server</span></code> object’s methods.</p>
<p>For example, if you were using HTTP/REST:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">ip</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&quot;127.0.0.1&quot;</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="o">!</span><span class="n">client</span><span class="p">.</span><span class="n">serverLive</span><span class="p">())</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;No server detected. Starting locally...</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="n">server</span><span class="p">.</span><span class="n">emplace</span><span class="p">();</span><span class="w"></span>
<span class="w">  </span><span class="n">server</span><span class="p">.</span><span class="n">value</span><span class="p">().</span><span class="n">startHttp</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">http_port</span><span class="p">);</span><span class="w"></span>
<span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">client</span><span class="p">.</span><span class="n">serverLive</span><span class="p">())</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="k">throw</span><span class="w"> </span><span class="n">amdinfer</span><span class="o">::</span><span class="n">connection_error</span><span class="p">(</span><span class="s">&quot;Could not connect to server at &quot;</span><span class="w"> </span><span class="o">+</span><span class="w"></span>
<span class="w">                                   </span><span class="n">server_addr</span><span class="p">);</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<p>With gRPC:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">ip</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&quot;127.0.0.1&quot;</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="o">!</span><span class="n">client</span><span class="p">.</span><span class="n">serverLive</span><span class="p">())</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;No server detected. Starting locally...</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="n">server</span><span class="p">.</span><span class="n">emplace</span><span class="p">();</span><span class="w"></span>
<span class="w">  </span><span class="n">server</span><span class="p">.</span><span class="n">value</span><span class="p">().</span><span class="n">startGrpc</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">grpc_port</span><span class="p">);</span><span class="w"></span>
<span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">client</span><span class="p">.</span><span class="n">serverLive</span><span class="p">())</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="k">throw</span><span class="w"> </span><span class="n">amdinfer</span><span class="o">::</span><span class="n">connection_error</span><span class="p">(</span><span class="s">&quot;Could not connect to server at &quot;</span><span class="w"> </span><span class="o">+</span><span class="w"></span>
<span class="w">                                   </span><span class="n">server_addr</span><span class="p">);</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<p>The native C++ API does not need starting and just creating the Server object is sufficient.</p>
<p>If the server is already running somewhere, you don’t need to do this.</p>
</section>
<section id="create-the-client-object">
<h2>Create the client object<a class="headerlink" href="#create-the-client-object" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">amdinfer::Client</span></code> base class defines how to communicate with the server over the supported protocols.
This client protocol is based on KServe’s API.
Each protocol inherits from this class and implements its methods.
Some examples of clients that you can create:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// vitis.cpp</span>
<span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">http_port_str</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">to_string</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">http_port</span><span class="p">);</span><span class="w"></span>
<span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">server_addr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;http://&quot;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">args</span><span class="p">.</span><span class="n">ip</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="s">&quot;:&quot;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">http_port_str</span><span class="p">;</span><span class="w"></span>
<span class="n">amdinfer</span><span class="o">::</span><span class="n">HttpClient</span><span class="w"> </span><span class="n">client</span><span class="p">{</span><span class="n">server_addr</span><span class="p">};</span><span class="w"></span>
</pre></div>
</div>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// tfzendnn.cpp</span>
<span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">grpc_port_str</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">to_string</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">grpc_port</span><span class="p">);</span><span class="w"></span>
<span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">server_addr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">args</span><span class="p">.</span><span class="n">ip</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="s">&quot;:&quot;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">grpc_port_str</span><span class="p">;</span><span class="w"></span>
<span class="n">amdinfer</span><span class="o">::</span><span class="n">GrpcClient</span><span class="w"> </span><span class="n">client</span><span class="p">{</span><span class="n">server_addr</span><span class="p">};</span><span class="w"></span>

<span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">amdinfer</span><span class="o">::</span><span class="n">Server</span><span class="o">&gt;</span><span class="w"> </span><span class="n">server</span><span class="p">;</span><span class="w"></span>
<span class="c1">// +start protocol</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">ip</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&quot;127.0.0.1&quot;</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="o">!</span><span class="n">client</span><span class="p">.</span><span class="n">serverLive</span><span class="p">())</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;No server detected. Starting locally...</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="n">server</span><span class="p">.</span><span class="n">emplace</span><span class="p">();</span><span class="w"></span>
<span class="w">  </span><span class="n">server</span><span class="p">.</span><span class="n">value</span><span class="p">().</span><span class="n">startGrpc</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">grpc_port</span><span class="p">);</span><span class="w"></span>
<span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">client</span><span class="p">.</span><span class="n">serverLive</span><span class="p">())</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="k">throw</span><span class="w"> </span><span class="n">amdinfer</span><span class="o">::</span><span class="n">connection_error</span><span class="p">(</span><span class="s">&quot;Could not connect to server at &quot;</span><span class="w"> </span><span class="o">+</span><span class="w"></span>
<span class="w">                                   </span><span class="n">server_addr</span><span class="p">);</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
<span class="c1">// -start protocol:</span>

<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Waiting until the server is ready...</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span><span class="w"></span>
<span class="n">amdinfer</span><span class="o">::</span><span class="n">waitUntilServerReady</span><span class="p">(</span><span class="o">&amp;</span><span class="n">client</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// ptzendnn.cpp</span>
<span class="n">amdinfer</span><span class="o">::</span><span class="n">NativeClient</span><span class="w"> </span><span class="n">client</span><span class="p">;</span><span class="w"></span>

<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Starting server locally...</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span><span class="w"></span>

<span class="n">amdinfer</span><span class="o">::</span><span class="n">Server</span><span class="w"> </span><span class="n">server</span><span class="p">;</span><span class="w"></span>

<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Waiting until the server is ready...</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span><span class="w"></span>
<span class="n">amdinfer</span><span class="o">::</span><span class="n">waitUntilServerReady</span><span class="p">(</span><span class="o">&amp;</span><span class="n">client</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
</section>
<section id="load-a-worker">
<h2>Load a worker<a class="headerlink" href="#load-a-worker" title="Permalink to this headline">¶</a></h2>
<p>Once the server is ready, you have to load a worker to handle your request.
This worker, and any load-time parameters it accepts, are backend-specific.
After loading the worker, you get back an endpoint string that you use to make requests to this worker.
If a worker is already ready on the server or you already have an endpoint, then you don’t need to do this.</p>
<p>Here are some of the different workers you can start to perform inference on a ResNet50 model.</p>
<p>XModel - Vitis AI on AMD FPGA:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">amdinfer</span><span class="o">::</span><span class="n">RequestParameters</span><span class="w"> </span><span class="n">parameters</span><span class="p">;</span><span class="w"></span>
<span class="n">parameters</span><span class="p">.</span><span class="n">put</span><span class="p">(</span><span class="s">&quot;model&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">args</span><span class="p">.</span><span class="n">path_to_model</span><span class="p">);</span><span class="w"></span>
<span class="n">parameters</span><span class="p">.</span><span class="n">put</span><span class="p">(</span><span class="s">&quot;batch_size&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">args</span><span class="p">.</span><span class="n">batch_size</span><span class="p">);</span><span class="w"></span>
<span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">endpoint</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">client</span><span class="o">-&gt;</span><span class="n">workerLoad</span><span class="p">(</span><span class="s">&quot;xmodel&quot;</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">parameters</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>TF+ZenDNN - ZenDNN on AMD CPU:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">amdinfer</span><span class="o">::</span><span class="n">RequestParameters</span><span class="w"> </span><span class="n">parameters</span><span class="p">;</span><span class="w"></span>
<span class="n">parameters</span><span class="p">.</span><span class="n">put</span><span class="p">(</span><span class="s">&quot;model&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">args</span><span class="p">.</span><span class="n">path_to_model</span><span class="p">);</span><span class="w"></span>
<span class="n">parameters</span><span class="p">.</span><span class="n">put</span><span class="p">(</span><span class="s">&quot;input_size&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">args</span><span class="p">.</span><span class="n">input_size</span><span class="p">);</span><span class="w"></span>
<span class="n">parameters</span><span class="p">.</span><span class="n">put</span><span class="p">(</span><span class="s">&quot;output_classes&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">args</span><span class="p">.</span><span class="n">output_classes</span><span class="p">);</span><span class="w"></span>
<span class="n">parameters</span><span class="p">.</span><span class="n">put</span><span class="p">(</span><span class="s">&quot;input_node&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">args</span><span class="p">.</span><span class="n">input_node</span><span class="p">);</span><span class="w"></span>
<span class="n">parameters</span><span class="p">.</span><span class="n">put</span><span class="p">(</span><span class="s">&quot;output_node&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">args</span><span class="p">.</span><span class="n">output_node</span><span class="p">);</span><span class="w"></span>
<span class="n">parameters</span><span class="p">.</span><span class="n">put</span><span class="p">(</span><span class="s">&quot;batch_size&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">args</span><span class="p">.</span><span class="n">batch_size</span><span class="p">);</span><span class="w"></span>
<span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">endpoint</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">client</span><span class="o">-&gt;</span><span class="n">workerLoad</span><span class="p">(</span><span class="s">&quot;tfzendnn&quot;</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">parameters</span><span class="p">);</span><span class="w"></span>
<span class="n">amdinfer</span><span class="o">::</span><span class="n">waitUntilModelReady</span><span class="p">(</span><span class="n">client</span><span class="p">,</span><span class="w"> </span><span class="n">endpoint</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>MIGraphX - MIGraphX on AMD GPU:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">amdinfer</span><span class="o">::</span><span class="n">RequestParameters</span><span class="w"> </span><span class="n">parameters</span><span class="p">;</span><span class="w"></span>
<span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">timeout_ms</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1000</span><span class="p">;</span><span class="w">  </span><span class="c1">// batcher timeout value in milliseconds</span>

<span class="c1">// Required: specifies path to the model on the server for it to open</span>
<span class="n">parameters</span><span class="p">.</span><span class="n">put</span><span class="p">(</span><span class="s">&quot;model&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">args</span><span class="p">.</span><span class="n">path_to_model</span><span class="p">);</span><span class="w"></span>
<span class="c1">// Optional: request a particular batch size to be sent to the backend. The</span>
<span class="c1">// server will attempt to coalesce incoming requests into a single batch of</span>
<span class="c1">// this size and pass it all to the backend.</span>
<span class="n">parameters</span><span class="p">.</span><span class="n">put</span><span class="p">(</span><span class="s">&quot;batch&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">args</span><span class="p">.</span><span class="n">batch_size</span><span class="p">);</span><span class="w"></span>
<span class="c1">// Optional: specifies how long the batcher should wait for more requests</span>
<span class="c1">// before sending the batch on</span>
<span class="n">parameters</span><span class="p">.</span><span class="n">put</span><span class="p">(</span><span class="s">&quot;timeout&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">timeout_ms</span><span class="p">);</span><span class="w"></span>
<span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">endpoint</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">client</span><span class="o">-&gt;</span><span class="n">workerLoad</span><span class="p">(</span><span class="s">&quot;migraphx&quot;</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">parameters</span><span class="p">);</span><span class="w"></span>
<span class="n">amdinfer</span><span class="o">::</span><span class="n">waitUntilModelReady</span><span class="p">(</span><span class="n">client</span><span class="p">,</span><span class="w"> </span><span class="n">endpoint</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>After loading a worker, make sure it’s ready before attempting to make an inference.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">amdinfer</span><span class="o">::</span><span class="n">waitUntilModelReady</span><span class="p">(</span><span class="o">&amp;</span><span class="n">client</span><span class="p">,</span><span class="w"> </span><span class="n">endpoint</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
</section>
<section id="prepare-images">
<h2>Prepare images<a class="headerlink" href="#prepare-images" title="Permalink to this headline">¶</a></h2>
<p>Depending on the model, you may need to perform some preprocessing of the data before making an inference request.
For ResNet50, this preprocessing generally consists of resizing the image, normalizing its values, and possibly converting types but its exact implementation depends on the model and on what the worker expects.
The implementations of the preprocessing functions can be seen in the examples’ sources and they differ for each backend.</p>
<p>In these examples, you can pass a path to an image or to a directory to the executable.
This single path gets converted to a vector of paths containing just the path you passed in or paths to all the files in the directory you passed in and its passed to the <code class="docutils literal notranslate"><span class="pre">preprocess</span></code> function.
The file at each path is opened and stored in an <code class="docutils literal notranslate"><span class="pre">std::vector&lt;T&gt;</span></code>, where <code class="docutils literal notranslate"><span class="pre">T</span></code> depends on the data type that a backend works with.
Since there may be many images, <code class="docutils literal notranslate"><span class="pre">preprocess</span></code> returns an <code class="docutils literal notranslate"><span class="pre">std::vector&lt;std::vector&lt;T&gt;&gt;</span></code>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span><span class="w"> </span><span class="n">paths</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">resolveImagePaths</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">path_to_image</span><span class="p">);</span><span class="w"></span>
<span class="n">Images</span><span class="w"> </span><span class="n">images</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">preprocess</span><span class="p">(</span><span class="n">paths</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
</section>
<section id="construct-requests">
<h2>Construct requests<a class="headerlink" href="#construct-requests" title="Permalink to this headline">¶</a></h2>
<p>Using the images after preprocessing, you can construct requests to the inference server.
For each image, you create an <code class="docutils literal notranslate"><span class="pre">InferenceRequest</span></code> and add input tensors to it.
The ResNet50 model only accepts a single input tensor so you just add one by specifying the image data, its shape, and data type.
In this example, you create a vector of such requests.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">amdinfer</span><span class="o">::</span><span class="n">InferenceRequest</span><span class="o">&gt;</span><span class="w"> </span><span class="n">requests</span><span class="p">;</span><span class="w"></span>
<span class="n">requests</span><span class="p">.</span><span class="n">reserve</span><span class="p">(</span><span class="n">images</span><span class="p">.</span><span class="n">size</span><span class="p">());</span><span class="w"></span>

<span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">initializer_list</span><span class="o">&lt;</span><span class="kt">uint64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="n">input_size</span><span class="p">,</span><span class="w"> </span><span class="n">input_size</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">};</span><span class="w"></span>

<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="n">image</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">images</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="n">requests</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">();</span><span class="w"></span>
<span class="w">  </span><span class="n">requests</span><span class="p">.</span><span class="n">back</span><span class="p">().</span><span class="n">addInputTensor</span><span class="p">((</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span><span class="n">image</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span><span class="w"> </span><span class="n">shape</span><span class="p">,</span><span class="w"></span>
<span class="w">                                 </span><span class="n">amdinfer</span><span class="o">::</span><span class="n">DataType</span><span class="o">::</span><span class="n">INT8</span><span class="p">);</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
</section>
<section id="make-an-inference">
<h2>Make an inference<a class="headerlink" href="#make-an-inference" title="Permalink to this headline">¶</a></h2>
<p>There are multiple ways of making a request to the inference server, some of which are used in the different implementations of these examples.
Before processing the response, you should verify it’s not an error.</p>
<p>Then, you can examine the outputs and, depending on the model, postprocess the results.
For ResNet50, the raw results from the inference server lists the probabilities for each output class.
The postprocessing identifies the highest probability classes and the top few of these are printed using the labels file to map the indices to a human-readable name.</p>
<p>Here are some examples of making an inference used in these examples:</p>
<p>This is the simplest way to make an inference: a blocking single inference where you loop through all the requests and make requests to the server one at a time.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="c1">// vitis.cpp</span>
<span class="w">  </span><span class="n">amdinfer</span><span class="o">::</span><span class="n">InferenceResponse</span><span class="w"> </span><span class="n">response</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">client</span><span class="p">.</span><span class="n">modelInfer</span><span class="p">(</span><span class="n">endpoint</span><span class="p">,</span><span class="w"> </span><span class="n">request</span><span class="p">);</span><span class="w"></span>
<span class="w">  </span><span class="n">assert</span><span class="p">(</span><span class="o">!</span><span class="n">response</span><span class="p">.</span><span class="n">isError</span><span class="p">());</span><span class="w"></span>

<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">amdinfer</span><span class="o">::</span><span class="n">InferenceResponseOutput</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="o">=</span><span class="w"></span>
<span class="w">    </span><span class="n">response</span><span class="p">.</span><span class="n">getOutputs</span><span class="p">();</span><span class="w"></span>
<span class="w">  </span><span class="c1">// for resnet50, we expect a single output tensor</span>
<span class="w">  </span><span class="n">assert</span><span class="p">(</span><span class="n">outputs</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span><span class="w"></span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="w"> </span><span class="n">top_indices</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">postprocess</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="n">args</span><span class="p">.</span><span class="n">top</span><span class="p">);</span><span class="w"></span>
<span class="w">  </span><span class="n">printLabel</span><span class="p">(</span><span class="n">top_indices</span><span class="p">,</span><span class="w"> </span><span class="n">args</span><span class="p">.</span><span class="n">path_to_labels</span><span class="p">,</span><span class="w"> </span><span class="n">image_path</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>You can also make a single asynchronous request to the server where you get back a <code class="docutils literal notranslate"><span class="pre">std::future</span></code> that you can use later to get the results of the inference.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="c1">// ptzendnn.cpp</span>
<span class="w">  </span><span class="n">amdinfer</span><span class="o">::</span><span class="n">InferenceResponseFuture</span><span class="w"> </span><span class="n">future</span><span class="w"> </span><span class="o">=</span><span class="w"></span>
<span class="w">    </span><span class="n">client</span><span class="p">.</span><span class="n">modelInferAsync</span><span class="p">(</span><span class="n">endpoint</span><span class="p">,</span><span class="w"> </span><span class="n">request</span><span class="p">);</span><span class="w"></span>
<span class="w">  </span><span class="n">amdinfer</span><span class="o">::</span><span class="n">InferenceResponse</span><span class="w"> </span><span class="n">response</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">future</span><span class="p">.</span><span class="n">get</span><span class="p">();</span><span class="w"></span>
<span class="w">  </span><span class="n">assert</span><span class="p">(</span><span class="o">!</span><span class="n">response</span><span class="p">.</span><span class="n">isError</span><span class="p">());</span><span class="w"></span>

<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">amdinfer</span><span class="o">::</span><span class="n">InferenceResponseOutput</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="o">=</span><span class="w"></span>
<span class="w">    </span><span class="n">response</span><span class="p">.</span><span class="n">getOutputs</span><span class="p">();</span><span class="w"></span>
<span class="w">  </span><span class="c1">// for resnet50, we expect a single output tensor</span>
<span class="w">  </span><span class="n">assert</span><span class="p">(</span><span class="n">outputs</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span><span class="w"></span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="w"> </span><span class="n">top_indices</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">postprocess</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="n">args</span><span class="p">.</span><span class="n">top</span><span class="p">);</span><span class="w"></span>
<span class="w">  </span><span class="n">printLabel</span><span class="p">(</span><span class="n">top_indices</span><span class="p">,</span><span class="w"> </span><span class="n">args</span><span class="p">.</span><span class="n">path_to_labels</span><span class="p">,</span><span class="w"> </span><span class="n">image_path</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>There are also some helper methods that wrap the basic inference APIs provided by the client.
The <code class="docutils literal notranslate"><span class="pre">inferAsyncOrdered</span></code> method accepts a vector of requests, makes all the requests asynchronously using the <code class="docutils literal notranslate"><span class="pre">modelInferAsync</span></code> API, waits until each request completes, and then returns a vector of responses.
If there are multiple requests sent in this way, they may be batched together by the server.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// migraphx.cpp</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">amdinfer</span><span class="o">::</span><span class="n">InferenceResponse</span><span class="o">&gt;</span><span class="w"> </span><span class="n">responses</span><span class="w"> </span><span class="o">=</span><span class="w"></span>
<span class="w">  </span><span class="n">amdinfer</span><span class="o">::</span><span class="n">inferAsyncOrdered</span><span class="p">(</span><span class="o">&amp;</span><span class="n">client</span><span class="p">,</span><span class="w"> </span><span class="n">endpoint</span><span class="p">,</span><span class="w"> </span><span class="n">requests</span><span class="p">);</span><span class="w"></span>
<span class="n">assert</span><span class="p">(</span><span class="n">num_requests</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">responses</span><span class="p">.</span><span class="n">size</span><span class="p">());</span><span class="w"></span>

<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0U</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">num_requests</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">amdinfer</span><span class="o">::</span><span class="n">InferenceResponse</span><span class="o">&amp;</span><span class="w"> </span><span class="n">response</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">responses</span><span class="p">[</span><span class="n">i</span><span class="p">];</span><span class="w"></span>
<span class="w">  </span><span class="n">assert</span><span class="p">(</span><span class="o">!</span><span class="n">response</span><span class="p">.</span><span class="n">isError</span><span class="p">());</span><span class="w"></span>

<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">amdinfer</span><span class="o">::</span><span class="n">InferenceResponseOutput</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="o">=</span><span class="w"></span>
<span class="w">    </span><span class="n">response</span><span class="p">.</span><span class="n">getOutputs</span><span class="p">();</span><span class="w"></span>
<span class="w">  </span><span class="c1">// for resnet50, we expect a single output tensor</span>
<span class="w">  </span><span class="n">assert</span><span class="p">(</span><span class="n">outputs</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span><span class="w"></span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="w"> </span><span class="n">top_indices</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">postprocess</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="n">args</span><span class="p">.</span><span class="n">top</span><span class="p">);</span><span class="w"></span>
<span class="w">  </span><span class="n">printLabel</span><span class="p">(</span><span class="n">top_indices</span><span class="p">,</span><span class="w"> </span><span class="n">args</span><span class="p">.</span><span class="n">path_to_labels</span><span class="p">,</span><span class="w"> </span><span class="n">paths</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          
                  <style>
                        .footer {
                        position: fixed;
                        left: 0;
                        bottom: 0;
                        width: 100%;
                        }
                  </style>
				  
				  <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="hello_world_echo.html" class="btn btn-neutral float-left" title="Hello World - Python" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="example_resnet50_python.html" class="btn btn-neutral float-right" title="Running ResNet50 - Python" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022 Advanced Micro Devices, Inc..
      <span class="lastupdated">Last updated on November 18, 2022.
      </span></p>
  </div>



										<div class="aem-Grid aem-Grid--16">
											<div class="aem-GridColumn aem-GridColumn--xxxlarge--none aem-GridColumn--xsmall--16 aem-GridColumn--offset--xsmall--0 aem-GridColumn--xlarge--none aem-GridColumn--xxlarge--none aem-GridColumn--default--none aem-GridColumn--offset--large--1 aem-GridColumn--xlarge--12 aem-GridColumn--offset--default--0 aem-GridColumn--xxlarge--10 aem-GridColumn--offset--xlarge--2 aem-GridColumn--offset--xxlarge--3 aem-GridColumn--offset--xxxlarge--4 aem-GridColumn--xsmall--none aem-GridColumn--large--none aem-GridColumn aem-GridColumn--large--14 aem-GridColumn--xxxlarge--8 aem-GridColumn--default--16">
												<div class="container-fluid sub-footer">

													                    <div class="row">
                        <div class="col-xs-24">
                          <p><a target="_blank" href="https://www.amd.com/en/corporate/copyright">Terms and Conditions</a> | <a target="_blank" href="https://www.amd.com/en/corporate/privacy">Privacy</a> | <a target="_blank" href="https://www.amd.com/en/corporate/cookies">Cookie Policy</a> | <a target="_blank" href="https://www.amd.com/en/corporate/trademarks">Trademarks</a> | <a target="_blank" href="https://www.amd.com/system/files/documents/statement-human-trafficking-forced-labor.pdf">Statement on Forced Labor</a> | <a target="_blank" href="https://www.amd.com/en/corporate/competition">Fair and Open Competition</a> | <a target="_blank" href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf">UK Tax Strategy</a> | <a target="_blank" href="https://docs.xilinx.com/v/u/9x6YvZKuWyhJId7y7RQQKA">Inclusive Terminology</a> | <a href="#cookiessettings" class="ot-sdk-show-settings">Cookies Settings</a></p>
                        </div>
                    </div>
												</div>
											</div>
										</div>
										
</br>


  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 <script type="text/javascript">
    $(document).ready(function() {
        $(".toggle > *").hide();
        $(".toggle .header").show();
        $(".toggle .header").click(function() {
            $(this).parent().children().not(".header").toggle(400);
            $(this).parent().children(".header").toggleClass("open");
        })
    });
</script>


</body>
</html>